\documentclass[11pt]{article}

\usepackage{apacite}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{lingmacros}
\usepackage{caption}
\usepackage{subcaption}

%\newcommand{\url}[1]{$#1$}

\definecolor{Blue}{RGB}{50,50,200}
\newcommand{\blue}[1]{\textcolor{Blue}{#1}}

\definecolor{Red}{RGB}{255,0,0}
\newcommand{\red}[1]{\textcolor{Red}{#1}}
\newcommand{\jd}[1]{\textcolor{Red}{[jd: #1]}} 

\definecolor{Green}{RGB}{50,200,50}
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  

 \newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}


\newcommand{\subsubsubsection}[1]{{\em #1}}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\tableref}[1]{Table \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\appref}[1]{Appendix \ref{#1}}
\newcommand{\sectionref}[1]{Section \ref{#1}}


\title{Over overinformativeness: rationally redundant referring expressions}

 
\author{{\large \bf Judith Degen, Caroline Graf, Robert X.D.~Hawkins, Noah D.~Goodman} \\
  \{jdegen,ngoodman\}@stanford.edu\\
  Department of Psychology, 450 Serra Mall \\
  Stanford, CA 94305 USA}

\begin{document}

\maketitle


\begin{abstract}
Referring is one of the most basic and prevalent uses of language. How do speakers choose from the wealth of referring expressions at their disposal? Rational theories of language use have come under attack for decades for not being able to account for the seemingly irrational overinformativeness ubiquitous in referring expressions. Here we present a novel production model of referring expressions within the Rational Speech Act framework that treats speakers as agents that rationally trade off cost and informativeness of utterances. Crucially, the assumption of deterministic meanings is relaxed. This allows us to capture a large number of seemingly disparate phenomena within one unified framework: the basic asymmetry in speakers' propensity to overmodify with color rather than size; the increase in overmodification in complex scenes; the increase in overmodification with atypical features; and the preference for basic level reference in nominal reference. The findings cast a new light on the production of referring expressions: rather than being wastefully overinformative, reference is rationally redundant. This implicates a production system geared towards communicative efficiency.

\textbf{Keywords:} 
reference; referring expressions; informativeness; probabilistic pragmatics; experimental pragmatics
\end{abstract}

\tableofcontents

\section{Introduction}
\label{sec:intro}

Reference to objects is one of the most basic and prevalent uses of language. But how do speakers choose amongst the wealth of referring expressions they have at their disposal? How does a speaker choose whether to refer to an object as \emph{the animal}, \emph{the dog}, \emph{the dalmatian}, or \emph{the big mostly white dalmatian}? The context within which the object occurs  (other non-dogs, other dogs, other dalmatians) plays a large part in determining which features the speaker chooses to include in their utterance  -- speakers aim to be sufficiently informative to uniquely establish reference to the intended object. However, speakers' utterances often exhibit what has been claimed to be \emph{overinformativeness}: referring expressions are often more specific than necessary for establishing unique reference, and they are so in systematic ways. However, providing a unified theory for speakers' systematic patterns of overinformativeness has so far proved elusive.

This paper is concerned with  modeling precisely this choice of referring expression (RE). We restrict ourselves to definite descriptions of the form \emph{the (ADJ}?\emph{)}+ \emph{NOUN}, that is, noun phrases that minimally contain the definite determiner \emph{the} followed by a head noun. In addition, any number of adjectives may occur between the determiner and the noun.\footnote{In contrast, we will \emph{not} provide a treatment of pronominal referring expressions, indefinite descriptions, names, or definite descriptions with post-nominal modification, though we offer some speculative remarks on how the approach outlined here can be applied to these cases. } A model of these REs will allow us to unify two domains in language production that have been typically treated as separate, and that have typically been treated as interesting for different reasons: the production of so-called overmodified referring expressions on the one hand, which a lot of literature in language production has been devoted to \cite{herrmann1976, Pechmann1989, nadig2002,  Maes2004, Engelhardt2006, Arts2011, Koolen2011, rubiofernandez2016}; and the production of simple nominal expressions, which has so far mostly received attention in the concepts and categorization literature \cite{Rosch1973, Rosch1976}. In the following, we review some of the key phenomena and puzzles in each of these literatures which have for the most part been treated as unrelated. We then present a model of RE production within the Rational Speech Act \cite{frank2012} framework, which treats speakers as boundedly rational agents who optimize the tradeoff between utterance cost and  informativeness. Our key innovation is to relax the assumption that semantic truth functions are deterministic. \jd{one sentence here that inspires intuition, or a paragraph foreshadowing, making it seem like the obvious solution?} It is this crucial innovation that allows us to provide a unified explanation for a great number of seemingly disparate phenomena from the modified and nominal RE literature.

\subsection{Production of referring expressions: a case against rational language use?}

How should a cooperative speaker produce referring expressions? Grice, in his seminal work, provided some guidance by formulating his famous conversational maxims, intended as a guide to listeners' expectations about good speaker behavior \cite{grice1975}. His maxim of Quantity, consisting of two parts, requires of speakers to:

\begin{enumerate}
	\item \emph{Quantity-1:} Make your contribution as informative as s required (for the purposes of the exchange).
	\item \emph{Quantity-2:} Do not make your contribution more informative than is required.
\end{enumerate}

That is, speakers should aim to produce neither under- nor overinformative utterances. While much support has been found for the former \cite{lots of people}, speakers seem remarkably happy to systematically violate Quantity-2. In modified referring expressions, they routinely produce modifiers that do not uniquely establish reference (e.g., \emph{the small blue thumbtack} instead of \emph{the small thumbtack} in contexts like \figref{fig:sizesufficient} \cite{bla bla}). In simple nominal expressions, speakers routinely choose to refer to an object with a basic level term even when a superordinate level term would have been sufficient for establishing reference (e.g., \emph{the dog} instead of \emph{the animal} in contexts like \figref{fig:dogcontexts}d \cite{bla bla}).

These observations have posed a challenge for theories of language production, especially those positing rational language use (including the Gricean one): why this extra expenditure of useless effort? Why this seeming blindness to the level of informativeness requirement? Many have argued from these observations that speakers are in fact not economical \cite{bla}. Some have derived a built-in preference for referring at the basic level from considerations of \jd{bla} and \jd{bla} \cite{Rosch1976}. Others have argued for salience-driven effects on willingness to overmodify \cite{dutch guys}. In all cases, it is argued that informativeness cannot be the key factor in determining the content of speakers' referring expressions.

Here we revisit this claim and show that systematically relaxing the requirement of a deterministic semantics for referring expressions also systematically changes the informativeness of utterances. This results in a reconceptualization of what have been termed \emph{overinformative referring expressions} as \emph{rationally redundant referring expressions}. We begin by reviewing the phenomena of interest that a revised theory of definite referring expressions should be able to account for. 

\subsection{Modified referring expressions}
\label{sec:modified}


Most of the literature on overinformative referring expressions has been devoted to the use of overinformative modifiers in modified referring expressions. The prevalent observation is that speakers frequently do not include only the minimal modifiers required for establishing unique reference, but often also include redundant modifiers \cite{Pechmann1989, nadig2002,  Maes2004, Engelhardt2006, Arts2011, Koolen2011}. However, not all modifiers are created equal: there are systematic differences in the overmodification patterns observed for size adjectives (e.g., \emph{big, small}), color adjectives (e.g., \emph{blue, red}), material adjectives (e.g., \emph{plastic, wooden}), and many others. Here we review some of the intriguing patterns of overmodification that have plagued that literature, focusing for the most part on size and color.



\begin{figure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{pics/size-sufficient.png}
\caption{Size sufficient.}
\label{fig:sizesufficient}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\includegraphics[width=\textwidth]{pics/color-sufficient.png}
\caption{Color sufficient.}
\label{fig:colorsufficient}
\end{subfigure}
\caption{Example contexts where size vs.~color is sufficient for unique reference. A green border marks the intended referent.}
\label{fig:thumbtack}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{pics/design}
\caption{Example contexts in which different levels of reference are necessary for establishing unique reference to the target marked with a green border: sub (\emph{dalmatian}, a), basic (\emph{dog}, b, c), or super (\emph{animal}, d).}
\label{fig:dogcontexts}
\end{figure}

\subsubsection{Asymmetry in redundant use of color and size adjectives}
\label{sec:asymmetry}

 In \figref{fig:sizesufficient}, singling out the object with the green border requires only mentioning its size, as in \emph{the small thumbtack}. But it is now well-documented that speakers routinely include redundant color adjectives as in \emph{the small blue thumbtack}, which do not uniquely single out the intended referent in these kinds of contexts \cite{Pechmann1989,  Belke2002, gatt2011}. However, the same is not true for size: in contexts like \figref{fig:colorsufficient}, where color is sufficient for unique reference (\emph{the blue thumbtack}), speakers overmodify much more rarely with size. \tableref{tab:colorsizeasymmetry} shows proportions of color, size, and (overinformative) color-and-size mentions in conditions like those depicted in \figref{fig:thumbtack} across different experiments. In all cases there is a preference for overmodifying with color but not with size.\footnote{There is quite a bit of variation in the actual numbers. We will discuss this variation in the Discussion of  \sectionref{sec:rsaevaluationbasicscene}.}
 
 \begin{table}
 \caption{Proportions of minimally informative \emph{color} or \emph{size} and overinformative \emph{color\_size} mentions in color-sufficient vs.~size-sufficient conditions across experiments.\jd{keep filling in.}}
 	\begin{tabular}{l l l l l l l l}
	\toprule
	 & & \multicolumn{3}{c}{Color sufficient} & \multicolumn{3}{c}{Size sufficient} \\
	Study & Language & \emph{color} & \emph{size} & \emph{color\_size}  & \emph{color} & \emph{size} & \emph{color\_size} \\
	\midrule
	\citeA{Pechmann1989} & Dutch & 99 & 0 & 1 & 9 & 36 & 55 \\
	\citeA{gatt2011} & English & 92 & 0 & 8 & 3 & 17 & 80\\
 	\citeA{gatt2011} & Dutch & 90 & 0 & 10 & 0 & 21 & 79 \\	
 Our baseline study & English & 94 & 0 & 6 & 2 & 52 & 46 \\ 
	\bottomrule
	\end{tabular}
	\label{tab:colorsizeasymmetry}
 \end{table}

Explanations for this asymmetry have varied. \citeA{Pechmann1989} was the first to take the asymmetry as evidence for speakers following an incremental strategy of object naming: speakers initially start to articulate an adjective denoting a feature that listeners can quickly and easily recognize (i.e., color) before they have fully inspected the display and extracted the sufficient dimension. However, this would predict that speakers routinely should produce expressions like \emph{the blue small thumbtack}, which violate the preference for size adjectives to occur before color adjectives in English \cite{bla adjective ordering prefs}. While Pechmann did observe such violations in his dataset, most cases of overmodification did not constitute such violations, and he himself concludes that incrementality cannot (on its own) account for the asymmetry in speakers' propensity for overmodifying with color vs.~size.  


Another explanation for the asymmetry is that speakers try to produce modifiers that denote features that are reasonably easy for the listener to perceive, so that, even when a feature is not fully distinguishing in context, it at least serves to restrict the number of objects that could plausibly be considered the target. Indeed, there has been some support for the idea that overmodification can be beneficial to listeners by facilitating target identification \cite{Arts2011, rubiofernandez2016}.

\jd{try to find a quote from someone who says it's all just a matter of cost?}

There have been various attempts to capture the color-size asymmetry in computational natural language generation models. The earliest contenders for models of definite referring expressions like the Full Brevity algorithm \cite{Dale1989} or the Greedy algorithm \cite{Dale1989} focused only on discriminatory value -- that is, an utterance's informativeness -- in generating referring expressions, which resulted in an inability to capture the color-size asymmetry: the models only produced the minimally specified expressions. Subsequently, the Incremental algorithm \cite{dale1995} incorporated a preference order on features, with color ranked higher than size. The order is traversed and each encountered feature included in the expression if it serves to exclude at least one further distractor. This results in the production of overinformative color but not size adjectives. However, the resulting asymmetry is much greater than that evident in human speakers, and is deterministic rather than exhibiting the probabilistic production patterns that human speakers exhibit. More recently, the PRO model \cite{GattEtAl2013} has sought to integrate the observation that speakers seem to have a preference for including color terms with the observation that a preference does not imply the deterministic inclusion of said color term. The model is specifically designed to capture the color-size asymmetry: in a first step, the uniquely distinguishing property (if there is one) is first selected deterministically.  In a second step, an additional property is added probabilistically, depending on both a salience parameter associated with the additional property and a parameter capturing speakers' eagerness to overmodify. If both properties are uniquely distinguishing, a property is selected probabilistically depending on its associated salience parameter. The second step proceeds as before.

However, while the PRO model -- the most state-of-the-art computational model of human production of modified referring expressions -- can capture the color-size asymmetry in and of itself, it is neither flexible enough to be extended straightforwardly to other modifiers beyond color and size, nor can it straightforwardly be extended to capture the more subtle systematicity with which the preference to overmodify with color changes based on various features of context. We delve into these more subtle patterns in the next two sections before presenting our alternative model within the Rational Speech Act framework.

\subsubsection{Scene variation}
\label{sec:scenevariation}

Speakers' propensity to overmodify with color is highly dependent on features of the distractor objects in the context. In particular, as the variation present in the scene increases, so does the probability of overmodifying with color \cite{Davies2013, Koolen2013}. How exactly scene variation is quantified differs between papers. One very clear demonstration of the scene variation effect was given by \citeA{Koolen2013}, who quantified scene variation as the number of feature dimensions along which objects in a scene vary. Over the course of three experiments, they compared a low-variation condition in which objects never differed in color with a high-variation condition in which objects differed in type, color, orientation, and size. They consistently found higher rates of overmodification with color in the high-variation (28-27\%) than in the low-variation (4-10\%) conditions.

The effect of scene variation on propensity to overmodify has typically been explained as the result of the demands imposed on visual search: in low-variation scenes, it is easier to discern the discriminating dimensions than in high-variation scenes, where it may be easier to simply start naming features of the target that are salient \cite{Koolen2013}. 

The PRO model does not have a straightforward way of capturing the effect of scene variation on probability of overmodification. One way of doing so is to make the salience and overmodification parameters directly dependent on the amount of variation in the scene. However, this requires additional free parameters and makes the model prone to overfitting. \jd{elaborate? throw out?}

\subsubsection{Feature typicality}
\label{sec:colortypicality}


Overmodification with color has been shown to be systematically related to the typicality of the color for the object. Building on work by \citeA{sedivy2003aa}, \citeA{Westerbeek2015} (and more recently, \citeA{rubiofernandez2016}) have shown that the more typical a color is for an object, the less likely it is to be mentioned when not necessary for unique reference. For example, speakers never refer to a yellow banana as \emph{the yellow banana}, but they sometimes refer to a brown banana as \emph{the brown banana}, and they almost always refer to a blue banana as \emph{the blue banana}. In fact, color typicality and probability exhibit a linear negative correlation \cite{Westerbeek2015}. Similar typicality effects have been shown for other (non-color) properties. For example, \citeA{Mitchell2013} showed that speakers are more likely to include an atypical than a typical property (either shape or material) when referring to everyday objects like boxes when mentioning at least one property was necessary for unique reference. %, when the shape and material was atypical (e.g., heart-shaped, made of clay) than when it was typical (e.g., square, made of cardboard). 

Whether speakers are more likely to mention atypical properties over typical properties because they are more salient to \emph{them} or because they are trying to make reference resolution easier for the listener, for whom presumably these properties are also salient, is an open question \cite{Westerbeek2015}. Some support for the audience design account comes from a study by \citeA{Huettig2011}, who found that listeners, after hearing a noun with a diagnostic color (e.g., \emph{frog}), are more likely to fixate objects of that diagnostic color (green), indicating that typical object features are rapidly activated and aid visual search. Nevertheless, the benefit for listeners and the salience for speakers might simply be a happy coincidence and speakers might not, in fact, be designing their utterances for their addressees. We will remain agnostic about the underlying reason for typicality effects \jd{will we, though? the model assumes that typicality affects the literal listener, who speakers reason about, so in a sense we're making a strong audience design claim.}

Irrespective of the source of typicality effects, it is unclear how the PRO model could accommodate them. As for the scene variation effects, it is possible to make the salience and overmodification eagerness parameters directly dependent on the typicality of the feature value for the object the speaker wants to refer to. However, as mentioned above, in the absence of a principled motivation for the way in which these parameters interact, this is simply an exercise in model-fitting without adding explanatory value. In addition, one is left with the task of explaining how scene variation and typicality should interact. 

\subsection{Nominal referring expressions}
\label{sec:nominal}

A problem related to the issue of how many additional features to include in a modified referring expression, but which has received much less attention in the language production literature, is that of deciding at which taxonomic level to refer to an object to in a simple nominal expression. That is, even in the absence of adjectives, a referring expression can be more or less informative: \emph{the dalmatian} communicates more information about the object in question than \emph{the dog}, which in turn is globally more informative than \emph{the animal}. Thus, this choice can be considered analogous to the choice of adding more modifiers -- in both cases, the speaker has a choice of being more or less specific about the intended referent. However, the choice of reference level in simple nominal referring expressions is also interestingly different from that of adding modifiers in that there is no additional word-level cost associated with being more specific -- the choice is between different one-word utterances, not between utterances of different lengths (in words). 

Nevertheless, cost affects the choice of reference level: in particular, speakers prefer more frequent nouns over less frequent ones \cite{bla}, and they prefer shorter ones over longer ones \cite{bla}. This may go part of the way towards explaining the well-documented effect from the concepts and categorization literature that speakers prefer to refer at the \emph{basic level} \cite{Rosch1976, Tanaka1991}. That is, in the absence of other constraints, even when a superordinate level term would be sufficient for establishing reference (as in \figref{fig:dogcontexts}d), speakers prefer to say \emph{the dog} rather than \emph{the animal}. 

However, there are nevertheless cases  of contexts where either the superordinate (\figref{fig:dogcontexts}d) or the basic level (\figref{fig:dogcontexts}b and \figref{fig:dogcontexts}c) term would be sufficient for unique reference, where speakers prefer to use the subordinate level term \emph{the dalmatian}. This is the case when the object is a particularly good instance of the subordinate level term or a particularly bad instance of the basic level term. For example, penguins, which are rated as particularly atypical birds, are often referred to at the subordinate level \emph{penguin} rather than at the basic level \emph{bird}, despite the general preference for the basic level \cite{Jolicoeur1984}.

\subsection{Summary}
\label{sec:introsummary}

In sum, the production of modified and simple nominal referring expressions is governed by a rich interplay of many factors, including an utterance's informativeness, its cost relative to alternative utterances, and the typicality of an object or its features. We are here especially interested in cases where speakers appear to be overinformative -- either by adding more modifiers or by referring at a more specific level than necessary for establishing unique reference. A summary of the effects we will focus on in the remainder of the paper is provided in \tableref{tab:effects}.

\begin{table}
\caption{List of effects a theory of referring expression production should account for.}
\begin{tabular}{l l } %p{5.5cm} }
\toprule
Effect & Description \\ %& Reported by \\
\midrule
Color/size asymmetry & More redundant use of color adjectives than size adjectives \\ %&  \citeA{Pechmann1989, Engelhardt2006, gatt2011} \red{others}\\
%Number of distractors & More redundant use of color with increasing number of distractors & ?? \red{deutsch} \\
Scene variation & More redundant use of color adjectives with increasing scene variation \\ %& \citeA{davies2009, Koolen2013}\\
Color typicality & More redundant use of color adjectives with decreasing color typicality \\ %& \citeA{sedivy2003aa, Westerbeek2015, rubiofernandez2016}\\
\midrule
Basic level preference & Preference for basic level term when superordinate level term sufficient \\ %& \citeA{Rosch1976}\\
Subordinate level mention & Unnecessary use of sub level term when basic or super level sufficient \\ %& \citeA{Jolicoeur1984}\\
\bottomrule
\end{tabular}
\label{tab:effects}
\end{table}


To date, there is no theory to account for all of these different phenomena; and no model has attempted to unify overinformativeness in the domain of modified and nominal referring expressions. We touched on some of the explanations that have been proposed for these phenomena. We also highlighted where computational models have been proposed for individual phenomena, and how they fall short. In the next section, we present the Rational Speech Act modeling framework, within which we will provide precisely the kind of theory that can account for at least all of the phenomena listed here and holds great promise for scaling up to many other overinformativeness phenomena.  


\section{Modeling speakers' choice of referring expression}
\label{sec:models}

Here we propose an extension to the production component of the Rational Speech Act \cite<RSA>{frank2012, goodmanstuhlmueller2013} modeling framework. This extension provides a principled explanation for the phenomena reviewed in the previous section and  holds promise for being generalizable to many further production phenomena related to overinformativeness, which we discuss in the General Discussion. We proceed by first presenting the general framework in \sectionref{sec:basicrsa}, and show why the most basic model cannot account for any of the phenomena outlined above, due to its strong focus on maximizing the informativeness of one-word expressions under a deterministic semantics. In \sectionref{sec:modifiedmodel} we introduce the crucial innovation: relaxing the assumption of a deterministic semantics. We show that the model can qualitatively account both for speakers' asymmetric propensity to overmodify with color rather than with size and (in \sectionref{sec:modelkoolen}) for speakers' propensity to overmodify more with increasing scene variation. In \sectionref{sec:rsaevaluationbasicscene} we report an interactive  reference game experiment which functions as a quantitative test of the model. In \sectionref{sec:colortypicality} we explore how the model captures feature typicality effects. In \sectionref{sec:nominal} we apply the model to the choice of simple nominal referring expressions and show that the qualitative preference for referring at the basic level (and the exceptions from that rule) emerges from the interaction of informativeness, utterance cost, and typicality. We test the model on a second interactive reference game experiment that provides data for a quantitative test of the model. For all cases we report  -- modified and nominal referring expressions -- we find that intorducing non-determinism into the semantic truth functions results in excellent quantitative fits to the data.

\subsection{Basic RSA}
\label{sec:basicrsa}

As has been pointed out by \citeA{GattEtAl2013}, the basic Rational Speech Act model as formulated by \citeA{frank2012} cannot generate overinformative referring expressions for two reasons: first, it trivially cannot do so because it is limited to one-word utterances \cite<see also>{Baumann2014}. But even when allowing two-word (or $n$-word) utterances, the speaker's utility will never allow for producing more redundant than minimal referring expressions as long as words contribute non-negative costs to the overall utterance cost. To see this, and as a basis for the innovations introduced in \sectionref{sec:modifiedmodel} and \sectionref{sec:reflevelmodel} it is useful to reiterate the basic form of the model.

Intuitively, the production component of RSA aims to soft-maximize the utility of utterances, where utility is defined in terms of the contextual informativeness of an utterance, given each utterance's literal semantics. Formally, this is treated as a pragmatic speaker $S_1$ reasoning about a literal listener $L_0$, who can be described by the following formula:

\begin{equation}
P_{L_0}(o | u) \propto \denote{u}(o).
\end{equation}

The literal listener $L_0$ hears an utterance $u$ from the set of available one-word utterances $U$ in the context of a set of objects  $O$ and forms a distribution over the referenced object, $o \in O$. Here, $\denote{u}(o)$ is the deterministic lexical meaning of the utterance $u$ when applied to object $o$. That is, $P_{L_0}(o | u)$ returns a uniform distribution over all $o$ denoted by $u$. For example, in the context shown in \figref{fig:sizesufficient}, $U = \{\textrm{\emph{big}}, \textrm{\emph{small}}, \textrm{\emph{blue}}, \textrm{\emph{red}}\}$ and $O = \{o_{\textrm{big\_blue}}, o_{\textrm{big\_red}}, o_{\textrm{small\_blue}}\}$. The values of $P_{L_0}(o | u)$ for each $u$ are shown on the left in \tableref{tab:detliteral}.

\begin{table}
%\centering
\caption{Literal listener distributions $P_{L_0}(o | u)$ for each utterance $u$ in the context depicted in \figref{fig:sizesufficient}, allowing only one-word utterances (left) or one- and two-word utterances (right).}
\begin{tabular}{l r r r}
\toprule
& $o_{\textrm{big\_blue}}$ & $o_{\textrm{big\_red}}$ & $o_{\textrm{small\_blue}}$ \\
\midrule
\emph{big} & .5 & .5 & 0\\
\emph{small} & 0 & 0 & 1\\
\emph{blue} & .5 & 0 & .5\\
\emph{red} & 0 & 1 & 0\\
\bottomrule
\end{tabular}
\begin{tabular}{l r r r}
\toprule
& $o_{\textrm{big\_blue}}$ & $o_{\textrm{big\_red}}$ & $o_{\textrm{small\_blue}}$ \\
\midrule
\emph{big} & .5 & .5 & 0\\
\emph{small} & 0 & 0 & 1\\
\emph{blue} & .5 & 0 & .5\\
\emph{red} & 0 & 1 & 0\\
\emph{big blue} & 1 & 0 & 0\\
\emph{big red} & 0 & 1 & 0\\
\emph{small blue} & 0 & 0 & 1\\
\bottomrule
\end{tabular}
\label{tab:detliteral}
\end{table}

The pragmatic speaker in turn produces an utterance proportional to the utility of that utterance, where utility is a function of both the utterance's \emph{informativeness}  with respect to the literal listener $\ln P_{L_0}(o | u)$ and the utterance's \emph{cost} $c(u)$:

\begin{equation}
P_{S_1}(u | o) \propto e^{\lambda \ln P_{L_0}(o | u) - \beta_c c(u)}
\end{equation}

Both the informativeness and the cost term receive a weight.\footnote{In fact, \citeA{frank2012} did not include a cost weight in their formulation and since they ultimately assumed equal costs for all utterances, they made no use of the cost function. Subsequent work has shown that taking into account utterance cost is necessary for modeling certain interpretation phenomena like cost-based quantity implicatures \cite{degenfrankejaeger2013} and M-implicature \cite{bergen2015}. The cost function will become important for our purposes in a little while.}   Informativeness is weighted by $\lambda$. To understand the effect of $\lambda$, assume that costs are equal and the cost function can thus be disregarded. As $\lambda$ approaches infinity, the speaker increasingly only chooses utterances that maximize informativeness; if $\lambda$ is 0, informativeness is disregarded and the speaker chooses randomly from the set of all available utterances; if $\lambda$ is 1, the speaker probability-matches. For our example in \tableref{tab:detliteral}, if the speaker wants to refer to $o_{\textrm{small\_blue}}$ she has two semantically possible utterances, \emph{small} and \emph{blue}, where \emph{small} is twice as informative as \emph{blue}. She will produce \emph{small} with the following probabilities as $\lambda$ varies: $P_{S_1}(\textrm{\emph{small}} | o_{\textrm{small\_blue}} ; \lambda = \infty) = 1$, $P_{S_1}(\textrm{\emph{small}} | o_{\textrm{small\_blue}} ; \lambda = 1) = \frac{2}{3}$, $P_{S_1}(\textrm{\emph{small}} | o_{\textrm{small\_blue}} ; \lambda = 0) = \frac{1}{4}$. Similarly, if we ignore informativeness and focus only on costs, any asymmetry in costs will be exaggerated with increasing $\beta_c$, such that the speaker will choose the least costly utterance with higher and higher probability as $\beta_c$ increases.

As noted above, this model cannot generate redundant referring expressions for multiple reasons. One of these is trivial: $U$ only contains one-word utterances. We can ameliorate this easily by allowing complex two-word utterances. We assume an intersective semantics for complex utterances $u_{\textrm{complex}}$ consisting of two sub-utterances $u_{\textrm{size}} \in \{\textrm{\emph{big}}, \textrm{\emph{small}}\}$ and $u_{\textrm{color}} \in \{\textrm{\emph{blue}}, \textrm{\emph{red}}\}$, such that $\denote{u_{\textrm{complex}}} = \denote{u_{\textrm{size}}} \wedge\denote{u_{\textrm{color}}}$. The resulting literal listener distributions are shown on the right in \tableref{tab:detliteral}. 

Does this now allow for generating redundant referring expressions? To answer this, let's turn again to the case where the speaker wants to communicate the small blue object. There are now two  utterances, \emph{small} and \emph{small blue}, which are both more informative than \emph{blue} and equally informative to each other, for referring to the small blue object. Because they are equally informative in context, what we need is for the complex utterance to be the \emph{cheaper} one in order to tilt the scales in its favor. While this achieves the desired effect mathematically, the cognitive plausibility of complex utterances being cheaper than simple utterances is highly dubious. But this is the only circumstance under which overinformative referring expressions will be produced with a greater probability than minimally specified referring expressions. Thus, unless we want to introduce a highly dubious cost assumption into the model, we must look elsewhere to account for overinformativeness: to the computation of informativeness itself. This is what we turn to next.

\subsection{RSA with non-deterministic semantics -- emergent color-size asymmetry}
\label{sec:modifiedmodel}

Here we introduce the crucial innovation: rather than assuming a deterministic truth-conditional semantics that returns 1 (true) or 0 (false) for any combination of expression and object, we assume a non-deterministic semantics that can return intermediate values. That is, rather than assuming that an object is unambiguously big or unambiguously blue, we allow for a non-deterministic semantics, capturing that objects count as big or blue to  varying degrees \jd{mention prototype theory and cite?}. In particular, consider some of the notable differences between color and size adjectives: color adjectives are considered  \emph{absolute adjectives} while size adjectives are inherently \emph{relative} \cite{kennedymcnally2005}. That is, while both size and color adjectives are vague, size adjectives are arguably context-dependent in a way that color adjectives are not -- whether an object is big depends inherently on its comparison class; whether an object is red does not.\footnote{This is not entirely true, as has been pointed out by \red{cite}: red hair has a very different color than red wine, which in turn has a different color from a red bell pepper. If presented out of context, only the last red is likely to be judged as red \cite{is there a ref?}. For our purposes, it suffices that one can give a color judgment but not a size judgment for an object presented in isolation.} In addition, color as a property has been claimed to be inherently salient in a way that size is not \cite{arts2011,gattetal2013}. Finally, we have shown in recent work that color adjectives are much less subjective in their interpretation than size adjectives \cite{scontrasunderreview}. We use these observations as motivation for exploring the effects of the assumption that the semantics of size adjectives is inherently noisier than the semantics of color adjectives.


Formally, $\denote{u}(o) = \textrm{exp}(\textrm{fidelity}(u,o))$, where $\textrm{fidelity(u,o)}$ returns a number between 0 and 1. The higher an utterance type's fidelity, the less noisy it is and the more likely it is to correctly pick out objects with the denoted property. The lower an utterance type's fidelity, the noisier the utterance is and the more likely it is to incorrectly pick out objects that don't exhibit the denoted property. We defer a discussion of the meaning of these fidelity values to the Discussion in \sectionref{sec:modifierdiscussion}. \jd{or straight to the GD?}

The effects of assuming non-deterministic truth functions in contexts like those depicted in \figref{fig:sizesufficient} and \figref{fig:colorsufficient} are visualized in \figref{fig:basicasymmetry}.\footnote{Here we show the results for $\lambda = 30$  and no utterance cost (i.e., $\beta_c = 0$). For a visualization of model behavior under varying $\lambda$s, see \appref{app:modelexploration}.} To orient the reader to the graph: the standard truth-functional semantics of the utterances are approximated where both fidelities are close to 1 (.999, right-most edge of each graph). In this case, the simple sufficient and complex redundant utterance are equally likely around .5 (because they are both equally informative and we are ignoring costs), and all other utterances are highly unlikely. The interesting question is under which circumstances, if any,  the standard color-size asymmetry emerges: redundant \emph{size-color} utterances are more likely than sufficient utterances where the fidelity of the sufficient dimension is lower than the fidelity of the insufficient dimension, for fidelities greater than .5. 

Let's focus on a particular example. Assume the context in \figref{fig:sizesufficient}, where size is sufficient for uniquely singling out the target. If color fidelity is high (e.g., .999, dark blue line) and size fidelity is relatively high, but not as high as color fidelity (e.g., .8 on x-axis), the probability of the redundant \emph{size-color} utterance \emph{small blue thumbtack} is $\approx .8$ and the probability of the simple \emph{size} utterance \emph{small thumbtack} is $\approx .2$. If we assume the same fidelity values for color and size in the color sufficient context in \figref{fig:colorsufficient}, the probability of the redundant \emph{size-color} utterance is $\approx .05$ and the probability of the simple \emph{color} utterance \emph{blue thumbtack} is $\approx .95$. Thus, when size adjectives are noisier than color adjectives, the model produces overinformative referring expressions with color, but not with size -- precisely the pattern observed in the literature. Indeed, these particular values are very similar to those found by \citeA{gatt2011}. Note also that no difference in adjective \emph{cost} is necessary for obtaining the overinformativeness asymmetry. However, assuming a greater cost for size than for color does further increase the observed asymmetry. We defer a discussion of costs to \sectionref{sec:exp1-scenevar}, where we infer the best parameter values (size and color cost and fidelity) given data from a reference game experiment.

A final observation regarding the probability of producing the insufficient utterance (e.g., \emph{blue thumbtack} in the size sufficient contexts in \figref{fig:sizesufficient}). The probability of producing the insufficient utterance is very high where its fidelity is high and the fidelity of the sufficient utterance is intermediate. This is because intermediate fidelity values lead to an utterance being randomly interpreted correctly or incorrectly; that is, \emph{small thumbtack} with  fidelity($u_{\textrm{size}}) = .5$ will apply equally to big and small objects in the context. The effect of this is that the literal listener returns a uniform distribution over all three objects in context upon observing \emph{small thumbtack}, adding no information. In contrast, a literal listener that observes \emph{blue thumbtack} assigns equal probability to the target and the color competitor, but lower probability to the distractor. Thus, even though the literal listener cannot distinguish between target and color competitor, the increased probability of correctly choosing the target by chance, due to the reduced probability of choosing the distractor, warrants the use of the insufficient \emph{blue thumbtack} utterance.  

\begin{figure}
\includegraphics[width=\textwidth]{pics/modelexploration-fidelityeffect-unlogged-wide}
\caption{Probability of producing insufficient, sufficient, and redundant utterance in contexts as depicted in \figref{fig:sizesufficient} and \figref{fig:colorsufficient}, as a function of fidelity of sufficient and insufficient utterance type (for $\lambda = 30$ and $ \beta_c = 0$).}
\label{fig:basicasymmetry}
\end{figure}

To summarize, we have thus far shown that RSA with non-deterministic adjective semantics can give rise to the well-documented color-size asymmetry in the production of overinformative referring expressions when size adjectives are noisier than color adjectives. But this basic asymmetry is only one of many intriguing patterns in the literature on referring expressions, including effects of scene variation and feature typicality, discussed in the Introduction. We turn to these phenomena next.

\subsection{RSA with non-deterministic semantics -- scene variation}
\label{sec:modelkoolen}

As discussed above, increased scene variation has been shown to increase overinformativeness, but scene variation can be quantified in many different ways. For concreteness sake we simulate the conditions reported by \citeA{Koolen2013}, who quantified scene variation as the number of feature dimensions along which pieces of furniture in a scene varied: type (e.g., chair, fan), size (big, small), and color (e.g., red, blue).\footnote{They also included orientation (left-facing, right-facing) as a dimension along which objects could vary in certain cases. We ignore this dimension here for simplicity's sake -- we simply wish to demonstrate that the model does indeed predict increased color redundancy with an increase in number of dimensions along which there is variation.} In particular, we  simulate the high and low variation conditions from their Experiments 1 and 2, reproduced in \figref{fig:koolencontexts}. 

In both conditions in both experiments, color was not necessary for establishing reference; that is, color mentions were always redundant. The two experiments differed in the dimension necessary for unique reference. In Exp.~1, only type was necessary (\emph{fan} and \emph{couch} in the low and high variation conditions in \figref{fig:koolencontexts}, respectively). In Exp.~2, size and type were necessary (\emph{big chair} and \emph{small chair} in \figref{fig:koolencontexts}, respectively). \citeA{Koolen2013} found lower rates of redundant color use in the low variation conditions (4\% and 9\%) than in the high variation conditions (24\% and 18\%).

We generated model predictions for precisely these four conditions. Note that by adding the type dimension as a distinguishing dimension, we must allow for an additional type fidelity parameter.

\begin{figure}
\centering
%\includegraphics[width=.9\textwidth]{pics/Koolen2013-exp1}
%\includegraphics[width=.9\textwidth]{pics/Koolen2013-exp2}
\includegraphics[width=\textwidth]{pics/koolen-conditions}
\caption{Contexts from Koolen et al.'s low variation (left column) and high variation (right column) conditions in Exp.~1 (top row) and Exp.~2 (bottom row).}
\label{fig:koolencontexts}
\end{figure}

\citeA{Koolen2013} counted any mention of color as a redundant mention. In Exp.~1, this includes the simple redundant utterances like \emph{blue couch} as well as complex redundant utterances like \emph{small blue couch}. In Exp.~2, where size was necessary for unique reference, only the complex redundant utterance \emph{small brown chair} was truly redundant.  The results of simulating these conditions for $\lambda = 30$, $ \beta_c = c(u_{\textrm{size}}) = c(u_{\textrm{color}}) = 1$, fidelity($u_{\textrm{size}}) = .8$, fidelity($u_{\textrm{color}}) = .999$, fidelity($u_{\textrm{type}}) = .9$ are shown in \figref{fig:koolensimulationresults}.\footnote{See \appref{app:koolenexploration} for a visualization of model predictions under a fuller exploration of parameter combinations.}


\begin{figure}
\centering
\includegraphics[width=.4\textwidth]{pics/koolen-effect}
\caption{Model predicted probability of redundant color utterance in Koolen conditions for $\lambda = 30$, $ \beta_c = c(u_{\textrm{size}}) = c(u_{\textrm{color}}) = 1$, fidelity($u_{\textrm{size}}) = .8$, fidelity($u_{\textrm{color}}) = .999$, fidelity($u_{\textrm{type}}) = .9$.}
\label{fig:koolensimulationresults}
\end{figure}

For both experiments, the model retrieves the empirically observed effect of variation on the probability of redundant color mention: when variation is greater, redundant color mention is more likely. Note that the absolute values predicted by the model ($\approx$ 8\% to $\approx$ 75\%) are different from the values observed by \citeA{Koolen2013}  ($\approx$ 4\% to $\approx$ 24\%). This need not concern us here: our goal was to investigate whether, using the same parameter values that best fit the few data points from the \citeA{gatt2011} study, the model predicts the qualitative effect of scene variation on redundancy. Indeed it does. 

Differences in exact values may stem from various sources. First, the best $\lambda$ value to assume may differ from experiment to experiment. Second, fidelity values may differ between experiments. Indeed, assuming a lower color fidelity of .9 maintains the qualitative effects but lowers to highest probability of redundancy to .26. Importantly, the basic requirements to yield the empirical scene variation effect are that size, type, and color fidelities follow the following ranking: fidelity($u_{\textrm{size}}$) $\leq$ fidelity($u_{\textrm{type}}$) $<$ fidelity($u_{\textrm{color}}$). If type fidelity is greater than color fidelity, the probability of redundantly mentioning color is close to zero and does not differ between variation conditions. This is because in those cases, color mention reduces, rather than adding, information about the target. Third, the values reported by \citeA{Koolen2013} were averaged over many different items -- here, we only reported model predictions for the example items they reported.

These results are encouraging: RSA not only predicts a systematic color-size asymmetry in propensity to redundantly produce adjectives when size is noisier than color; it also predicts that there should be more redundant color mention as the number of dimensions along which objects in the scene vary increases. However, thus far we have only probed the model for qualitative effects from very few data points previously reported in the literature. Independently evaluating the utility of the model requires  testing it on a large dataset. This is what we turn to next.

\section{Non-deterministic RSA for modified referring expressions}
\label{sec:rsaevaluationbasicscene}

Adequately assessing the explanatory value of RSA with non-deterministic truth functions requires evaluating how well it does at predicting the probability of various types of utterances in large datasets of naturally produced referring expressions. To this end we proceed in two steps. First we report the results of a web-based interactive reference game in which we systematically manipulate scene variation (in a somewhat different way than \citeA{Koolen2013} did). We then perform Bayesian data analysis to generate model predictions, conditioning on the observed production data. This will both allow us a) to assess  how likely the model is to generate the actually observed data -- i.e., to obtain a measure of model quality -- and b) to infer the posterior probability of parameter values -- i.e., to understand whether the assumed asymmetries in adjective fidelity and/or cost discussed in the previous section are warranted.


\subsection{Experiment 1: scene variation in modified referring expressions}
\label{sec:exp1-scenevar}

We saw in \sectionref{sec:modelkoolen} that non-deterministic RSA correctly predicts effects of scene variation on redundant adjective use. In particular, we saw that color is more likely to be used redundantly as the number of dimensions along which objects in a scene vary increases. However, we would like to a) go beyond a qualitative investigation of scene variation effects and also b) ask whether redundant size mention is also affected by scene variation. The notion of scene variation we employ is the proportion of distractor items that do not share the value of the insufficient feature with the target, that is, as the number of distractors $n_{\textrm{diff}}$ that differ in the value of the insufficient feature divided by the total number of distractors $n_{\textrm{total}}$:

\begin{equation*}
	\textrm{scenevar} = \frac{n_{\textrm{diff}}}{n_{\textrm{total}}}
\end{equation*}

To explain, let's turn again to \figref{fig:sizesufficient}. Here, the target item is the small blue thumbtack and there are two distractor items: a big blue thumbtack and a big red thumbtack. Thus, for the purpose of establishing unique reference, size is the sufficient dimension and color the insufficient dimension. There is one distractor that differs from the target in color (the big red thumbtack) and there are two distractors in total. That is, $\textrm{scenevar} = \frac{1}{2} = .5$. Scene variation is minimal when all distractors are of the same color as the target, in which case it is 0. Scene variation is maximal when all distractors except for one (in order for the dimension to remain insufficient for establishing reference) are of a different color than the target. That is, scene variation may take on values between 0 and $\frac{n_{\textrm{total} - 1}}{n_{\textrm{total}}}$, i.e, approaching but never reaching 1.

Using the same parameter values as in the previous two model explorations ($\lambda = 30$, $ \beta_c = c(u_{\textrm{size}}) = c(u_{\textrm{color}}) = 1$, fidelity($u_{\textrm{size}}) = .8$, fidelity($u_{\textrm{color}}) = .999$), we generate model predictions for size-sufficient and color-sufficient contexts, varying scene variation by varying number of distractors (2, 3, or 4) and number of distractors that don't share the insufficient feature value. The resulting model predictions are shown in \figref{fig:numdistractors}: the probability of redundant adjective use increases with increasing scene variation when size is sufficient, but not when color is sufficient. This can be explained by noise distributions in the literal listener across contexts: in size-sufficient contexts, as the number of distractors of a different color than the target increases, using the relatively noiseless color term in addition to the more noisy size term reduces uncertainty about the target object. However, the same is not true of the color-sufficient contexts: there is very little uncertainty about the target upon observing the minimal color utterance -- adding the size term only introduces more uncertainty about the target, regardless of the amount of scene variation. For slightly lower color fidelities a small increase in redundant size use is also predicted. In general: increased scene variation is predicted to lead to more redundant adjective use for less noisy adjectives.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{pics/scenevariation-effect}
\caption{Probability of minimal and redundant utterance as a function of scene variation and sufficient dimension (for $\lambda = 30$, $ \beta_c = c(u_{\textrm{size}}) = c(u_{\textrm{color}}) = 1$, fidelity($u_{\textrm{size}}) = .8$, fidelity($u_{\textrm{color}}) = .999$).}
\label{fig:numdistractors}
\end{figure}


To test non-deterministic RSA predictions, we conducted an interactive web-based written production study within a reference game setting.\footnote{See \appref{app:replication}  for a validation of the general paradigm, in which we qualitatively replicate the findings of \citeA{gatt2011} with a different set of stimuli.} Speakers and listeners were shown arrays of objects of that could vary in color and size. Speakers were asked to produce a referring expression to allow the listener to identify a target object. We manipulated the number of distractor objects in the grid, as well as the variation in color and size among distractor objects.




\begin{figure}
\begin{subfigure}{\textwidth}
\includegraphics[width=\textwidth]{pics/speaker-perspective-small.png}
\caption{Speakers' perspective}
\label{fig:speakerpersp}
\end{subfigure}

\begin{subfigure}{\textwidth}
\includegraphics[width=\textwidth]{pics/listener-perspective-small.png}
\caption{Listeners' perspective.}
\label{fig:listenerpersp}
\end{subfigure}
\caption{Example displays from the  (a) speaker's and the  (b)  listener's perspective on a \emph{size-sufficient 4-2} trial.}
\label{fig:speakerlistenerperspective}
\end{figure}

\subsubsection{Method}

\paragraph{Participants}

We recruited 58 pairs of participants (116 participants total) over Amazon's Mechanical Turk who were each paid \$1.75 for their participation. Data from another 7 pairs who prematurely dropped out of the experiment and who could therefore not be compensated for their work, were also included. Here and in all other experiments reported in this paper, participants' IP address was limited to US addresses and only participants with a past work approval rate of at least 95\% were accepted. 

\paragraph{Procedure}

Participants were paired up through a real-time multi-player interface \cite{Hawkins15_RealTimeWebExperiments}. For each pair, one participant was assigned the speaker role and one the listener role. They  initially received written instructions that informed participants that one of them would be the Speaker and the other the Listener. They were further told that they would see some number of objects on each round and that the speaker's task is to communicate one of those objects, marked by a green border, to the listener. They were explicitly told that using locative modifiers would be useless because the order of objects on their partner's screen would be different than on their own screen. Before continuing to the experiment, participants were required to correctly answer a series of questions about the experimental procedure. These questions are listed in \appref{app:numdistractors}.

On each trial participants saw an array of objects. The array contained the same objects for both speaker and listener, but the order of objects was randomized and was typically different for speaker and listener. In the speaker's display, one of the objects -- henceforth the \emph{target} -- was highlighted with a green border. See \figref{fig:speakerlistenerperspective} for an example of the listener's and speaker's view on a particular trial.

The speaker produced a referring expression to communicate the target to the listener by typing in a chat window. After pressing Enter or clicking the `Send' button, the speaker's message was shown to the listener. The listener then clicked on the object they thought was the target, given the speaker's message.  Once the listener clicked an object, a red border appeared around that object in both the listener and the speaker's display for 1 second before advancing to the next trial.

Both speakers and listeners could write in the chat window, allowing listeners to request clarification if necessary. Listeners could only click on an object and advance to the next trial once the speaker had sent a message. 


\paragraph{Materials}

Participants proceeded through 72 trials. Of these, half were critical trials of interest and half were filler trials. On critical trials, we varied the feature that was sufficient to mention for uniquely establishing reference, the total number of objects in the array, and  the number of objects that shared the non-sufficient feature with the target. 

Objects varied in color and size. On 18 trials, color was a sufficient property for distinguishing the target. On the other 18 trials, size was sufficient. See \figref{fig:speakerlistenerperspective} for an example of a size-sufficient trial from both the speaker's and the listener's perspective. 

We further varied the amount of variation the scene by varying the number of distractor objects in each array (2, 3, or  4) and the number of distractors that did shared the non-sufficient feature value with the target. That is, when size was the sufficiently distinguishing property, we varied the number of distractors that shared the same color as the target. This number had to be at least one, since otherwise the non-sufficient property would have been sufficient for uniquely establishing reference, i.e.~it would not have been redundant to mention it. Each total number of distractors was crossed with each possible number of distractors that shared the non-sufficient property, leading to the following nine conditions: \emph{2-1, 2-2, 3-1, 3-2, 3-3, 4-1, 4-2, 4-3,} and \emph{4-4}, where the first number indicates the total number and the second number the shared number of distractors. Each condition occurred twice with each sufficient dimension. Objects never differed in type within one array (e.g., all objects are thumbtacks in \figref{fig:speakerlistenerperspective} but always differed in type across trials. Each object type could occur in two different sizes and two different colors. We deliberately chose photo-realistic objects of intuitively fairly typical colors. The 36 different object types and the colors they could occur with are listed in \appref{app:itemtypes}. 


Fillers were target trials from Exp.~2, a replication of \cite{graf2016}. Each filler item contained a three-object grid. None of the filler objects occurred on target trials. Objects stood in various taxonomic relations to each other and required neither size nor color mention for unique reference. See \sectionref{sec:exp2-reflevel} for a description of these materials.

\subsubsection{Data pre-processing and exclusion}

We collected data from 2171 critical trials. Of these, 33 (1.5\%) were excluded because the listener did not select the target. 

Because we did not restrict participants' utterances in any way, they produced many different kinds of referential expressions. Testing the model's predictions required, for each trial, either excluding it or classifying the produced utterance as an instance of a \emph{color}-only mention, a \emph{size}-only mention, or a \emph{color-and-size} mention. To this end we conducted the following semi-automatic data pre-processing. 

%In a first step, an R script automatically checked whether (what the experimenters deemed to be) the target's color or size was included in the utterance. In this way, \red{XXX \%} of cases were classified as containing a size or color term. However, this did not capture that sometimes, a participant produced a different color or size term than the one we had intended (e.g., \emph{pink} instead of \emph{purple} \red{XXX \%} or \emph{large} instead of \emph{big} \red{XXX \%}) or the expression contained a typo (e.g., \emph{pruple} instead of \emph{purple} \red{XXX \%}). In a second step, one of the authors (CG) therefore manually checked the automatic coding: utterances of an unintended color or size were coded as an instance of the intended color or size if they were similar enough in meaning and utterances with typos were corrected. Most of the time, participants converged on a convention of mentioning simply the target's size and/or color, e.g., \emph{purple} or \emph{big blue}, without even using an article (e.g., \emph{the}) or mentioning the object's type (e.g., \emph{comb}). Articles were omitted in 93.2 \% of cases and object types were omitted in 74.2 \% of cases. We did not analyze this any further.

First, 33 trials on which the listener selected the wrong referent were excluded, leading to the elimination of 1.5\% of trials. Then, an R script automatically checked whether the speaker's utterance contained a precoded color (i.e. \emph{black, blue, brown, gold, green, orange, pink, purple, red, silver, violet, white, yellow}) or size (i.e. \emph{big, bigger, biggest, huge, large, larger, largest, little, small, smaller, smallest, tiny}) term. In this way, 95.7 \% of cases were classified as mentioning size and/or color. However, this did not capture that sometimes, participants produced meaning-equivalent modifications of color/size terms for instance by adding suffixes (e.g., \emph{bluish}), using abbreviations (e.g., \emph{lg} for \emph{large} or \emph{purp} for \emph{purple}), or using non-precoded color labels (e.g., \emph{lime} or \emph{lavender}). Expressions containing a typo (e.g., \emph{pruple} instead of \emph{purple}) could also not be classified automatically. In the next step, one of the authors (CG) therefore manually checked the automatic coding to include these kinds of modifications in the analysis. This caught another 1.5\% of trials. Most of the time, participants converged on a convention of mentioning simply the target's size and/or color, e.g., \emph{purple} or \emph{big blue}, without even using an article (e.g., \emph{the}) or mentioning the object's type (e.g., \emph{comb}). Articles were omitted in 93.1 \% of cases and object types were omitted in 71.5 \% of cases. We did not analyze this any further.

There were 50 cases (2.3\%) in which the speaker made reference to the distinguishing dimension in an abstract way, e.g.~\emph{different color}, \emph{unique one}, \emph{ripest}, \emph{very girly}, or \emph{guitar closest to viewer}. While interesting as utterance choices,\footnote{Certain participants seemed to have deliberately used this as a strategy even though simply mentioning the distinguishing property would have been shorter in most cases. In all, only 12 participants produced these kinds of utterances: one 18 times, one 8 times, one 6 times, two 3 times, one 2 times, and the remaining six only once each.} these cases were excluded from the analysis. There were 3 cases that were nonsensical, e.g. \emph{bigger off a shade}, which were also excluded. Finally, there were 6 cases where only the insufficient dimension was mentioned -- these were excluded from the analysis reported in the next section, where we are only interested in minimal or redundant utterances, not underinformative ones, but were included in the Bayesian data analysis reported in \sectionref{sec:modifiermodeleval}. After the exclusion, 2079 cases classified as one of \emph{color}, \emph{size}, or \emph{color-and-size} entered into the analysis.

%\jd{Caroline, is there anything else we should mention here?}caroline:Should we mention that we included lighter/darker mentions as color mentions? (There are ~10; especially used to describe turtles and avocados)

\subsubsection{Results}
\label{sec:modelempiricalresults}
% The analysis R script is in writing/2016/theory/rscripts/analysis_modifiers.R

Proportions of redundant \emph{color-and-size} and minimal \emph{color} or \emph{size} utterances are shown in \figref{fig:exp1results} alongside model results (to be explained further in \sectionref{sec:modifiermodeleval}). There are three main questions of interest: first, do we replicate the color/size asymmetry in probability of redundant adjective use? Second, do we replicate the previously established effect of increased redundant color use with increasing scene variation? Third, is there an effect of scene variation on redundant size use and if so, is it smaller compared to that on color use, as is predicted under asymmetric color and size adjective fidelities?

We addressed all of these questions in one fell swoop by conducting a mixed effects logistic regression analysis predicting redundant over minimal adjective use from fixed effects of sufficient property (color vs.~size), scene variation (proportion of distractors that does not share the insufficient property value with the target), and the interaction between the two. The model included the maximal random effects structure that allowed the model to converge: by-speaker and by-item random intercepts as well as by-speaker random slopes for scene variation. 

We observed a main effect of sufficient property such that speakers were more likely to redundantly use color than size adjectives ($\beta = 3.61$, $SE = .23$, $p < .0001$), replicating the much-documented color-size asymmetry. We further observed a main effect of scene variation such that redundant adjective use increased with increasing scene variation ($\beta = 4.11$, $SE = .49$, $p < .0001$). Finally, we also observed a significant interaction between sufficient property and scene variation ($\beta = 3.03$, $SE = .81$, $p < .0002$). Simple effects analysis revealed that the interaction is driven by the scene variation effect being much smaller in the \emph{color-sufficient} condition ($\beta = 2.59$, $SE = .78$, $p < .0009$) than in the \emph{size-sufficient} condition ($\beta = 5.63$, $SE = .45$, $p < .0001$), as predicted under the assumption that size modifiers are noisier than color modifiers.



\begin{figure}
\centering
%\includegraphics[width=\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/scenevariation-fixed-reducedconditions}
\includegraphics[width=\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/scenevariation-fixed-reducedconditions-unlogged}
\caption{Empirical utterance proportions  (red)  alongside point-wise maximum a posteriori (MAP) estimates of the model's posterior predictives for utterance probability (blue) as a function of scene variation. Rows indicate the sufficient dimension, columns the produced utterance. Here and in all following plots, error bars indicate 95\% bootstrapped confidence intervals.}
\label{fig:exp1results}
\end{figure}


\subsection{Model evaluation: scene variation}
\label{sec:modifiermodeleval}

% Extra info: 10000 samples, burn 3000, uniform drift, acceptance rate .4

We performed Bayesian Data Analysis to generate model predictions and infer likely parameter values, conditioning on the observed production data (coded into \emph{size}, \emph{color}, and \emph{size-and-color} utterances as described above) and integrating over the following free parameters: color fidelity $f_{\textrm{c}}$, size fidelity $f_{\textrm{s}}$, color cost $c_{\textrm{color}}$, size cost $c_{\textrm{size}}$, cost weight $\beta_{\textrm{cost}}$, and speaker rationality parameter $\lambda$. We assumed uniform priors for each parameter: $f_{\textrm{c}} \sim \mathcal{U}(0,1)$, $f_{\textrm{s}} \sim \mathcal{U}(0,1)$, $c_{\textrm{color}} \sim \mathcal{U}(0,2)$, $c_{\textrm{size}} \sim \mathcal{U}(0,2)$, $\beta_{\textrm{cost}} \sim \mathcal{U}(0,10)$, $\lambda  \sim \mathcal{U}(0,40)$.
We implemented both the cognitive and data-analysis models in the probabilistic programming language WebPPL \cite{GoodmanStuhlmuller14_DIPPL}.
Inference for the cognitive model was exact, while we used Markov Chain Monte Carlo (MCMC) to infer posteriors for the six free parameters.

Point-wise maximum a posteriori (MAP) estimates of the model's posterior predictives for each combination of utterance, sufficient dimension, number of distractors, and number of different distractors (collapsing across different items) are compared to empirical data in \figref{fig:modelexp1scatter}. At this level, the model achieves a correlation of $r = .99$. Looking at results additionally on the by-item level yields a correlation of $r = .85$. The model thus does a very good job of capturing the quantitative patterns in the data. This can also be seen in \figref{fig:exp1results}, where model predictions are plotted alongside the empirical proportions by condition. The only clear flaw is that the model predicts greater redundant adjective use than empirically observed when there is no scene variation at all. %\jd{a sentence on why?}.

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/predictives-collapsed-fixed-reducedconditions-unlogged}
\caption{Scatterplot of point-wise maximum a posteriori (MAP) estimates of the model's posterior predictives against empirical proportions ($r=$.99).}
\label{fig:modelexp1scatter}
\end{figure}

Parameter posteriors are shown in \figref{fig:modifierparamposteriors}. Crucially, the fidelity of color is inferred to be higher than that of size -- there is no overlap between the 95\% highest density intervals (HDIs) for the two parameters. % \red{the following footnote is deprecated and only applies to logged fidelities}\footnote{Analysis of the cases where color fidelity was inferred to be lower than size fidelity reveals that these are also the cases where the cost weight is high -- that is, where the model tries to compensate by increasing the effect of the size-color cost asymmetry (recall that size is always more costly than color). See also \appref{app:fidelity-outliers} for a more detailed analysis.} 
That is, size modifiers are inferred to be noisier than color modifiers. The relatively high inferred $\lambda$ suggests that this difference in fidelity contributes substantially to the observed color-size asymmetries in redundant adjective use. As for cost, there is a lot of overlap in the inferred cost of size and color modifiers, suggesting that no cost difference is necessary to obtain the color-size asymmetry and the scene variation effects. These results are compatible with previous claims \red{cite cite} that part of the explanation for the color-size asymmetry stems from the low cognitive cost involved in producing color modifiers compared to size modifiers. However, the results do suggest that a cost asymmetry is not the driving force behind the asymmetry in redundant adjective use. Note further that the asymmetry cannot be reduced to cost differences: in \sectionref{sec:modifiedmodel} we showed that the color-size asymmetry in redundant adjective use requires an asymmetry in modifier fidelity. An asymmetry in cost only serves to further enhance the asymmetry brought about by the fidelity asymmetry, but cannot carry the redundant use asymmetry on its own.
%\red{the following cost discussion is deprecated and applies only to logged fidelities} As for cost, the cost of size modifiers is inferred to be roughly twice that of color, with a non-zero inferred weight on cost. This provides some support for previous claims \red{cite cite} that part of the explanation for the color-size asymmetry stems from the low cognitive cost involved in producing color modifiers compared to size modifiers. However, note that the asymmetry cannot be reduced to cost differences: indeed in \sectionref{sec:modifiedmodel} we showed that the color-size asymmetry in redundant adjective use requires an asymmetry in modifier fidelity. An asymmetry in cost only serves to further enhance the asymmetry brought about by the fidelity asymmetry, but cannot carry the redundant use asymmetry on its own.


\begin{figure}
\centering
%\includegraphics[width=\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/parameterposteriors-fixed-reducedconditions}
\includegraphics[width=\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/parameterposteriors-fixed-reducedconditions-unlogged}
%logged fidelity: \caption{Posterior distribution over model parameters. Maximum a posteriori (MAP)  $f_{\textrm{s}}$ = 0.86, 95\% highest density interval (HDI) = [0.84,0.88]; MAP $f_{\textrm{c}}$ = 0.90, HDI = [0.85,0.93]; MAP $c_{\textrm{size}}$ = .91, HDI = [.75, 1.9]; MAP $c_{\textrm{color}}$ = 0.36, HDI = [0.31,0.85]; MAP $\beta_c$ = 2.2, HDI = [1.7,4.6]; MAP $\lambda$ = 10.5, HDI = [9.6,11.3]}
\caption{Posterior distribution over model parameters. Maximum a posteriori (MAP)  $f_{\textrm{s}}$ = 0.79, 95\% highest density interval (HDI) = [0.76,0.80]; MAP $f_{\textrm{c}}$ = 0.88, HDI = [0.86,0.91]; MAP $c_{\textrm{size}}$ = .08, HDI = [0, 1.5]; MAP $c_{\textrm{color}}$ = 0.07, HDI = [0,0.9]; MAP $\beta_c$ = 0.04, HDI = [0,1.6]; MAP $\lambda$ = 34.0, HDI = [30.8,36.5]}
\label{fig:modifierparamposteriors}
\end{figure}

\subsection{Discussion}
\label{sec:modifierdiscussion}

As should be apparent from this section, non-deterministic RSA provides an excellent fit to data of freely produced modified referring expressions. In particular, we have shown that the crucial element in obtaining the much-documented color-size asymmetry in the propensity to overmodify is that the semantic truth functions of size adjectives be noisier than those of color adjectives. Asymmetries in cost of adjectives  only serve to further enhance the asymmetries resulting from asymmetries in utterance fidelity. In addition, we showed that asymmetric effects of scene variation on overmodification are also well captured by non-deterministic RSA: scene variation leads to a greater increase in overmodification with less noisy than with more noisy modifiers.

\jd{Interesting snippet from Pechmann which we may want to quote: ``in information theory, a positive function is assigned to redundancy, since it can compensate for partial loss of information. Can overspecification in referential communication have the same function? We can rule out this possibility, because we regard those utterances as being `redundant' which include nondistinguishing features in addition to distinguishing ones. Yet nondistinguishing features cannot compensate for the loss of distinguishing information, since by definition nondistinguishing features do not distinguish the target referent from context. At best, they can reduce the domain of referential alternatives.'' $\rightarrow$ But that IS compensating!}


\jd{already bring up discussion points here? eg is it really about COLOR vs SIZE or particular color and size terms (blue vs turqoise), or about COLOR/blue for a particular object (ie typicality)? what about the issue that regularly comes up: that in context, surely these modifiers are not noisy?}

\jd{There is quite a bit of variation in the actual overmodification numbers in previous studies. Use that as a bridge to typicality and next section (you also say in intro that you're going to discuss this, so you better do it).} 



\section{Feature typicality}
\label{sec:colortypicality}

In \sectionref{sec:rsaevaluationbasicscene} we showed that non-deterministic successfully captures both the basic asymmetry in overmodification with color vs.~size as well as effects of scene variation, quantified in various different ways. But in \sectionref{sec:colortypicality} we discussed a further characteristic of speakers' overmodification behavior: speakers are more likely to redundantly produce modifiers that denote atypical rather than typical object features, i.e., they are more likely to refer to a blue banana as a \emph{blue banana} rather than as a \emph{banana}, and they are more likely to refer to a yellow banana as a \emph{banana} than as a \emph{yellow banana} \cite{sedivy2003a, Westerbeek2015}. Non-deterministic RSA as we have set it up thus far does not capture this asymmetry: it knows that a particular modifier is a color modifier with a particular fidelity; it does not know anything about the typicality of the denoted properties for the referent. 

We would like to warn and disillusion the reader upfront: we will not solve the problem of how to get overmodification behavior from the typicality of features compositionally. This is a problem for all theories of modification \cite{kamp1995}. However, we would like to offer a proof of concept showing that, if the non-determinism in the RSA semantics is not at the adjective type (color, size) level, but instead at the level of combinations of referring expressions and objects, the model produces precisely the sorts of typicality effects reported in the literature. 

Let us elaborate. Where before we took a fidelity to be a number between 0 and 1 indicating how likely a type of modifier (size, color) was to correctly apply to an object, we now treat it as indicating how good an instance of a particular referring expression the object in question is. For example, take the banana case: assume three contexts of objects with yellow, brown, and blue objects. Assume further that one of the objects is a banana, and the only difference between the three contexts is whether the banana is blue, brown, or yellow. In every context there is another object of the same color as the banana, so color is redundant, while there are no other bananas, so object category mention is sufficient for reference. Assume further the fidelity values shown in \tableref{tab:colorobjectfidelities}. These values should be read as follows: a yellow banana is a very good or typical instance of a\emph{banana} -- \emph{banana} applied to yellow bananas has a high fidelity of .9. In contrast, brown bananas are less typical instances of \emph{banana}s (.35), and blue bananas are highly atypical \emph{banana}s (.1) but still better than objects of an other non-banana type (.015). Going along the diagonal, you can see that we assume for each remaining utterance that its fidelity is very high (.99) when applied to an object in its extension and very low otherwise (.015).

\begin{table}
\centering
\caption{Hypothetical fidelity values for utterances (rows) as applied to objects (columns).}
\begin{tabular}{l l l l l}
\toprule
Utterance & yellow banana & brown banana & blue banana & other\\
\midrule
\emph{banana} & .9 & .35 & .1 & .015 \\
\emph{yellow banana} & .99 & .015 & .015 & .015 \\
\emph{brown banana} & .015 & .99 & .015 & .015 \\
\emph{blue banana} & .015 & .015 & .99 & .015 \\
\emph{other} & .015 & .015 & .015 & .99 \\
\bottomrule
\end{tabular}
\label{tab:colorobjectfidelities}
\end{table}

With $\lambda$ = 12 and $\beta_c$ = 5 (that is, both informativeness and utterance cost receive a substantial weight), the resulting speaker probabilities for the (minimal) \emph{banana} are .99, .37, and .05, respectively, to refer to the yellow banana, the brown banana, and the blue banana. In contrast, the resulting speaker probabilities for the redundant \emph{yellow banana}, \emph{brown banana}, and \emph{blue banana} are .01, .63, and .95, respectively. That is, redundant color mention increases with decreasing fidelity of the simple \emph{banana} utterance.

So far we have shown that non-deterministic RSA can capture typicality effects in principle if we assume that fidelity does not operate at the adjective type level but instead captures the typicality of an object for the alternative (minimal and redundant) referring expressions. If an object is more typical for the redundant expression than for the minimal expression, then the bigger the difference in typicality, the greater the relative informativeness of the redundant expression, and the greater the probability of it being produced.

We can now ask whether taking into account this more fine-grained notion of non-deterministic semantics plays a role in the dataset collected in Exp.~1. 
%\sectionref{sec:exp1-scenevar}. 
A note upfront: the stimuli for Exp.~1 were specifically designed to be realistic objects; that is, very low typicality values or even a large degree of variation in typicality would be surprising. Nevertheless, it is plausible that typicality differences exist. If they do, there are two interesting questions to ask: first, do we replicate the typicality effects reported in the literature -- that is, are less typical objects more likely to lead to redundant adjective use than more typical objects? Second, does including empirically elicited typicality values at the object-utterance level further improve the quality of the RSA model? We address the first question in \sectionref{sec:modifiertypicalityeffects} and the second question in \sectionref{sec:modelevalcolortypicality}.

\subsection{Experiment 1a: Typicality effects in Exp.~1}
\label{sec:modifiertypicalityeffects}

To assess whether we replicate the color typicality effects previously reported in the literature \cite{sedivy2003a, Westerbeek2015, rubiofernandez2016}, we elicited color typicality norms for each of our items and then included typicality as an additional predictor of redundant adjective use in the regression analysis reported previously. 
%See \appref{app:typicalityexperiment} for a detailed description of the methodology and materials.

\subsubsection{Methods}

\paragraph{Participants}

We recruited 60 participants over Amazon's Mechanical Turk who were each paid \$0.25 for their participation.

\paragraph{Procedure and materials}

On each trial, participants saw one of the big versions of the items used in Exp.~1 and were asked to answer the question ``How typical is this for an \emph{X}?'' on a continuous slider with endpoints labeled ``very atypical'' to ``very typical.'' \emph{X} was a referring expression consisting of either only the correct noun (e.g., \emph{stapler}) or the noun modified by the correct color (e.g., \emph{red stapler}). \figref{fig:modifiertypstimulus} shows an example of a modified trial.

Each participant saw each of the 36 objects once. An object was randomly displayed in one of the two colors it occurred with in Exp.~1 and was randomly displayed with either the correct modified utterance or the correct unmodified utterance, in order to obtain roughly equal numbers of object-utterance combinations.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{pics/redstapler.png}
\caption{A modified example trial from the typicality elicitation experiment.}
\label{fig:modifiertypstimulus}
\end{figure}

Importantly, we only elicited typicality norms for unmodified utterances and utterances with color modifiers, but not utterances with size modifiers. This was because it is virtually impossible to obtain size typicality norms for objects presented in isolation, due to the inherently relational nature of size adjectives. Consequently, we only test for the effect of typicality on \emph{size-sufficient} trials.


\subsubsection{Results and discussion}

We coded the slider endpoints as 0 (``very atypical'') and 1 (``very typical''), essentially treating each response as a typicality value between 0 and 1. For each combination of object, color, and utterance (modified/unmodified), we computed that item's mean. Mean typicalities were generally lower for unmodified than for modified utterances: mean typicality for unmodified utterances was .67 (sd=.17, mode=.76) and for modified utterances .75 (sd=.12, mode=.81). This can also be seen on the left in \figref{fig:typicalitydists}. Note that, as expected given how the stimuli were constructed, typicality was generally skewed towards the high end, even for unmodified utterances. This means that there was not much variation in  the difference in typicality between modified and unmodified utterances. We will refer to this difference as \emph{typicality gain}, reflecting the overall gain in typicality via color modification over the unmodified baseline. As can be seen on the right in \figref{fig:typicalitydists}, in most cases typicality gain was close to zero.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{pics/typicality-dists}
\caption{Typicality densities for modified and unmodified utterances (left) and histogram of typicality gains (differences between modified and unmodified typicalities, right).}
\label{fig:typicalitydists}
\end{figure}

This makes the typicality analysis difficult: if typicality gain is close to zero for most cases (and, taking into account confidence intervals, effectively zero), it is hard to evaluate the effect of typicality on redundant adjective use. In order to maximize power, we therefore conducted the analysis only on those items for which for at least one color the confidence intervals for the modified and unmodified utterances did not overlap. There were only four such cases: \emph{(pink) golfball}, \emph{(pink) wedding cake}, \emph{(green) chair}, and \emph{(red) stapler}, for a total of 231 data points.

Predictions differ for size-sufficient and color-sufficient trials. Given the typicality effects reported in the literature and the predictions of non-deterministic RSA, we expect greater redundant color use on size-sufficient trials with \emph{increasing} typicality gain. The predictions for redundant size use on color-sufficient trials are unclear from the previous literature. Non-deterministic RSA, however,  predicts greater redundant size use with \emph{decreasing} typicality gain: small color typicality gains reflect the relatively low out-of-context utility of color. In these cases, it may be useful to redundantly use a size modifier even if that modifier is noisy. If borne out, these predictions should surface in an interaction between sufficient property and typicality gain. Visual inspection of the empirical proportions of redundant adjective use in \figref{fig:maxtypicalitydiff} suggests that this pattern is indeed borne out.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{pics/maxtypicalitydiff}
\caption{Utterance probability for four items as a function of difference in typicality between modified and unmodified utterance (x-axis) and sufficient dimension (columns). }
\label{fig:maxtypicalitydiff}
\end{figure}

In order to investigate the effect of typicality gain on redundant adjective use, we conducted a mixed effects logistic regression analysis predicting redundant over minimal adjective use from fixed effects of scene variation, sufficient dimension, the interaction of scene variation and sufficient property, and the interaction of typicality gain and sufficient property. This is the same model as reported in \sectionref{sec:modelempiricalresults}, with the only difference that the interaction between sufficient property and typicality gain was added. All predictors were centered before entering the analysis. The model contained the maximal random effects structure that allowed it to converge: by-participant and by-item (where item was a color-object combination) random intercepts. 

The model summary is shown in \tableref{tab:colortypicalityresults}. We replicate the effects of sufficient property and scene variation observed earlier on this smaller dataset. Crucially, we observe a significant interaction between sufficient property and typicality gain.\footnote{Conducting the same analysis on the entire dataset (i.e., using all of the noisy typicality estimates, replicated the scene variation and sufficient property effects. The interaction of typicality gain and sufficient property went in the same direction numerically, but failed to reach significance ($\beta = 1.52$, $SE = 1.45$, $p < .29$).} Simple effects analysis reveals that this interaction is due to a positive effect of typicality gain on redundant adjective use in the size-sufficient condition ($\beta = 4.47$, $SE = 1.65$, $p < .007$) but a negative effect of typicality gain on redundant adjective use in the color-sufficient condition  ($\beta = -5.77$, $SE = 2.49$, $p < .03$). 

\begin{table}[!tbp]
\caption{Model coefficients, standard errors, and p-values. Significant p-values are bolded.}
\begin{center}
\begin{tabular}{lrrl}
\toprule
\multicolumn{1}{l}{}&\multicolumn{1}{c}{Coef $\beta$}&\multicolumn{1}{c}{SE($\beta$)}&\multicolumn{1}{c}{$p$}\tabularnewline
\midrule
Intercept&$-1.85$&$0.34$&\textbf{\textless .0001}\tabularnewline
Scene variation&$ 4.29$&$1.16$&\textbf{\textless .001}\tabularnewline
Sufficient property&$ 2.72$&$0.60$&\textbf{\textless .0001}\tabularnewline
Scene variation : Sufficient property&$ 0.88$&$2.12$&\textless 0.68\tabularnewline
Sufficient property : Typicality gain&$ 9.43$&$2.68$&\textbf{\textless .001}\tabularnewline
\bottomrule
\end{tabular}\end{center}
\label{tab:colortypicalityresults}
\end{table}

An important point is of note: the typicality elicitation procedure we employed here is somewhat different from that employed by \citeA{Westerbeek2015}, who asked their participants ``How typical is this color for this object?'' We did this for conceptual reasons: the values that go into the semantics of the RSA model are most easily conceptualized as the typicality of an object as an instance of an utterance. While the typicality of a feature for an object type no doubt plays into how good of an instance of the utterance the object is, deriving our typicalities from the  statistical properties of the subjective distributions of features over objects is beyond the scope of this paper. However, in a separate experiment we did ask participants the Westerbeek question. The correlation between mean typicality ratings from the Westerbeek version and the unmodified ``How typical is this for \emph{X}'' version was .75. The correlation between the Westerbeek version and the modified version was .64. The correlation between the Westerbeek version and typicality gain was -.52.

For comparison, including typicality means obtained via the Westerbeek question as a predictor instead of typicality gain on the four high-powered items replicated the significant interaction between typicality and sufficient property ($\beta = -6.77$, $SE = 1.88$, $p < .0003$). Simple effects analysis revealed that the interaction is again due to a difference in slope in the two sufficient property conditions: in the size-sufficient condition, color is less likely to be mentioned with increasing color typicality   ($\beta = -3.66$, $SE = 1.18$, $p < .002$), whereas in the color-sufficient condition, size is more likely to be mentioned with increasing color typicality ($\beta = 3.09$, $SE = 1.45$, $p < .04$).\footnote{Again, conducting this analysis on the entire dataset yielded only a marginal interaction of sufficient property and color typicality in the right direction ($\beta = -1.10$, $SE = .64$, $p < .09$).}

We thus overall find moderate evidence for typicality effects in our dataset. Typicality effects are strong for those items that clearly display typicality differences between the modified and unmodified utterance, but much weaker for the remaining items. That the evidence for typicality effects is relatively scarce is no surprise: the stimuli were specifically designed to minimize effects of typicality. However, the fact that both ways of quantifying typicality predicted redundant adjective use in the expected direction suggests that with more power or with stimuli that exhibit greater typicality variation, these effects may show up more clearly.

In the next section we evaluate whether the fit of non-deterministic RSA to the data is improved by using the empirically elicited typicalities as the values in the non-deterministic semantics.

\subsection{Model evaluation: color typicality}
\label{sec:modelevalcolortypicality}

\jd{insert actual numbers below! figure out what's going wrong in the bda that the returned color fidelities are lower than the returned size fidelities}

In order to evaluate the effect of utterance-object level typicality we proceed in two steps: first, we present the results of performing Bayesian data analysis in the same way as reported in \sectionref{sec:modifiermodeleval}, with the only difference that instead of fixed utterance type level fidelities we include the more fine-grained fidelity values corresponding to the typicality norms. We will focus more closely on the four items shown in \figref{fig:maxtypicalitydiff} in order to demonstrate the effect of including typicality in the model \red{once you know, actually put a summary sentence here instead of "in order to see how the model bla"}. In a second step we then address the pressing question of whether more fine-grained typicalities add any predictive value. 

\subsubsection{Model evaluation: empirical typicalities}

In order to generate model predictions and infer likely parameter values for the dataset reported above,  we repeated the same Bayesian data analysis procedure as described above, with one difference: instead of using the utterance-type level fidelities we fed the model the empirically elicited typicality norms. This was slightly less trivial than it sounds for two reasons. First, we only elicited typicality norms for object-utterance pairs for which the utterance was either just the simple object category noun (e.g., \emph{chair}) or the color-modified noun (e.g., \emph{green chair}); that is, we did not elicit size typicality norms (for reasons described in the previous section). Second, we did not elicit typicality norms for utterance-object pairs where the object would not be in the deterministic semantics' extension (e.g., \emph{green chair} used to refer to a red chair). In order to fill in the typicality gaps, so to speak, we assigned fidelity values to utterances as follows: object-utterance pairs where the object is in the extension of the utterance were assigned the empirically elicited typicality for that pair (e.g., \emph{chair} typicality for any chair, \emph{red chair} typicality for red chairs, see \tableref{tab:empiricalcolorfidelities}). If the object was not in the utterance extension, it received a fidelity of $1 - f_{c}$, where $f_c$ is an utterance type level fidelity parameter for color (as in the basic non-deterministic model reported above). For objects in the utterance extension where the utterance additionally contained a size modifier, the empirical fidelity was multiplied by an utterance type level size fidelity $f_{s}$. See \tableref{tab:empiricalcolorfidelities} for an overview of fidelity values for one item (red/green chair). 


\begin{table}
\centering
\caption{Fidelity values (rescaled) for one example item (chair) that occurred in two colors (red, green). Rows indicate different utterances, columns indicate different objects. See \tableref{tab:empiricalscaledunscaledtyps} for the raw and rescaled empirical typicality values for the red/green chair item.}
\begin{tabular}{l l l l l}
\toprule
Utterance & small red chair & small green chair & big red chair & big green chair\\
\midrule
\emph{chair} & .83 & .67 & .83 & .67 \\
\emph{red chair} & .84 & $1 - f_{\textrm{c}}$ & .84 & $1 - f_{\textrm{c}}$ \\
\emph{green chair} & $1 - f_{\textrm{c}}$ & .85 & $1 - f_{\textrm{c}}$ & .85 \\
\emph{small chair} & $f_{\textrm{s}} \cdot .83$ & $f_{\textrm{s}} \cdot .67$ & $(1 - f_{\textrm{s}}) \cdot .83$ & $(1 - f_{\textrm{s}}) \cdot .67$ \\
\emph{big chair} & $(1 - f_{\textrm{s}}) \cdot .83$ & $(1 - f_{\textrm{s}}) \cdot .67$ & $f_{\textrm{s}} \cdot .83$ & $f_{\textrm{s}} \cdot .67$ \\
\emph{small red chair} & $f_{\textrm{s}} \cdot .84$ & $f_{\textrm{s}} \cdot (1 - f_{\textrm{c}})$ & $(1 - f_{\textrm{s}}) \cdot .84$ & $(1-f_{\textrm{s}}) \cdot (1-f_{\textrm{c}})$ \\
\emph{big red chair} & $(1-f_{\textrm{s}}) \cdot .84$ & $(1-f_{\textrm{s}}) \cdot (1-f_{\textrm{c}})$ & $f_{\textrm{s}} \cdot .84$ & $f_{\textrm{s}} \cdot (1-f_{\textrm{c}})$ \\
\emph{small green chair} & $f_{\textrm{s}} \cdot (1-f_{\textrm{c}})$ & $f_{\textrm{s}} \cdot .85 $ & $(1-f_{\textrm{s}}) \cdot (1-f_{\textrm{c}})$ & $(1-f_{\textrm{s}}) \cdot .85$ \\
\emph{big green chair} & $(1-f_{\textrm{s}}) \cdot (1-f_{\textrm{c}})$ & .$(1-f_{\textrm{s}}) \cdot .85$ & $f_{\textrm{s}} \cdot (1-f_{\textrm{c}})$ & $f_{\textrm{s}} \cdot .85$ \\
\bottomrule
\end{tabular}
\label{tab:empiricalcolorfidelities}
\end{table}

One final point of note: empirically elicited typicality values were rescaled to range from 0.5 to 1 (instead of from 0 to 1). This was done because a fidelity value of .5, when there are only two potential feature values (e.g., two colors in the scene, red and green), leads to a random choice between green and red items; that is, the modifier contains no information. When the fidelity value is below .5 in these two-feature value scenarios, the modifier contextually acquires the meaning of the other feature dimension, e.g., \emph{green} is more likely to pick out red than green objects. The typicality values we elicited were not degree of membership values (which is what the model expects). Rather, they are more comparable to distance from prototype values \cite<see>[for a discussion of the difference]{kamp1995}. By rescaling the empirical typicality values to fall above .5, we guaranteed that the utterance would have at least an above chance of meaning what it would mean under a deterministic semantics. \tableref{tab:empiricalscaledunscaledtyps} exemplifies the effect of rescaling the raw typicality values for the red/green chair item.

\begin{table}
\centering
\caption{Raw and rescaled typicalities for the red and green chair items.}
\begin{tabular}{l l l l l}
\toprule
& \multicolumn{2}{c}{Raw} & \multicolumn{2}{c}{Rescaled}\\
Utterance & red chair & green chair & red chair & green chair\\
\midrule
\emph{chair} & .68 & .41 & .83 & .67\\
\emph{red chair} & .69 & NA & .84 & NA\\
\emph{green chair} & NA & .70  & NA & .85\\
\bottomrule
\end{tabular}
\label{tab:empiricalscaledunscaledtyps}
\end{table}

\paragraph{Results} Including typicality  yielded similar item-wise model-data correlations as the basic model \red{XXX}. Posteriors over parameters are shown in \figref{fig:typicalityparamposteriors}. \red{Discuss similarities/diffs to basic model.} 

\begin{figure}
\label{fig:typicalityparamposteriors}
%\includegraphics{bla}
\caption{\red{Posterior distribution over model parameters. Maximum a posteriori (MAP)  $f_{\textrm{s}}$ = 0.79, 95\% highest density interval (HDI) = [0.76,0.80]; MAP $f_{\textrm{c}}$ = 0.88, HDI = [0.86,0.91]; MAP $c_{\textrm{size}}$ = .08, HDI = [0, 1.5]; MAP $c_{\textrm{color}}$ = 0.07, HDI = [0,0.9]; MAP $\beta_c$ = 0.04, HDI = [0,1.6]; MAP $\lambda$ = 34.0, HDI = [30.8,36.5]}}
\end{figure}

Posterior predictives for the cases with greatest typicality gain -- \emph{chair}, \emph{golfball}, \emph{weddingcake}, and \emph{stapler} -- are shown in \figref{fig:ppmaxtypdiff} alongside the posterior predictives from the basic model (with utterance type level fidelities). In the basic non-deterministic model, probability of redundant utterances is similar for items of different colors. In the model that includes empirically elicited typicalities, the probability of redundant utterances is greater where typicality gain is greater; that is, for cases where the unmodified utterance has low typicality and the modified utterance high typicality, analogous to the \emph{blue banana} case.

\subsubsection{Model evaluation: interpolation analysis}

Because the correlations between model-predicted utterance probability posterior predictives and empirical proportions are very similar across the basic and empirical typicality model, the question arises whether utterance-level typicalities add any predictive value whatsoever. In order to address this question we present a second BDA analysis in which we introduce an additional parameter in the model that functions as a weight on fidelity type: if the weight is 0, only the utterance-type level fidelities are used; if the weight is 1 only the empirical typicalities are used. Therefore, if the BDA returns posterior values for fidelity type weight greater than 0, empirical typicalities are justified.

\paragraph{Results} Posteriors over parameters are shown in \figref{fig:interpolposteriors}. \red{insert figure} The MAP estimate for fidelity type weight is \red{XXX (HDI = [X,X])}, suggesting that utterance-level typicality adds predictive value.

\begin{figure}
\label{fig:interpolposteriors}
%\includegraphics{bla}
\caption{\red{Posterior distribution over model parameters. Maximum a posteriori (MAP)  $f_{\textrm{s}}$ = 0.79, 95\% highest density interval (HDI) = [0.76,0.80]; MAP $f_{\textrm{c}}$ = 0.88, HDI = [0.86,0.91]; MAP $c_{\textrm{size}}$ = .08, HDI = [0, 1.5]; MAP $c_{\textrm{color}}$ = 0.07, HDI = [0,0.9]; MAP $\beta_c$ = 0.04, HDI = [0,1.6]; MAP $\lambda$ = 34.0, HDI = [30.8,36.5]}}
\end{figure}

\subsection{Discussion}

\red{main points: a) that non-deterministic RSA with utterance-level typicalities as fidelity values captures the color typicality effects reported in the literature qualitatively (yellow/blue banana); b) that even in our dataset, where items were designed to not exhibit great typicality effects, we find evidence of typicality effects on utterance probability, replicating previous studies; and c) BDA shows that including empirically elicited typicality norms in the model adds predictive value. This suggests that speakers are tracking typicality at a very fine-grained level}.

\jd{This is all well and good, but to what extent is non-deterministic RSA just a model of modifier choice in modified referring expressions? Put differently, does non-deterministic RSA provide a good account of content selection in referring expressions more generally? To answer this question we move beyond modified referring expressions and turn to simple nominal referring expressions.}

In the next section we turn to extending non-deterministic RSA beyond the choice of modifier.

\section{Evaluating non-deterministic RSA for nominal choice}
\label{sec:nominal}

In this section we investigate whether non-deterministic RSA can account for referring expression production beyond the choice of modifier. To do so, we begin by presenting a  second production experiment. This experiment investigates speakers' choice of level of reference in nominal referring expression (\emph{dalmation}, \emph{dog}, or \emph{animal}). As discussed in \sectionref{sec:nominal},  multiple factors have been shown to play a role in the choice of nominal referring expression, including an expression's contextual informativeness, its cognitive cost (short and frequent terms are preferred over long and infrequent ones) \red{cite cite}, and its typicality (an utterance is more likely to be used if the object is a good example of it) \red{is that true? cite. yes, caroline put ref in cogsci talk}.   We then evaluate non-deterministic RSA on the nominal choice dataset by conducting the same type of Bayesian data analysis as reported in the previous section.

\subsection{Experiment 2: level of reference in nominal referring expressions}
\label{sec:exp2}

Exp~2 employed the same procedure as Exp.~1, but each display consisted of three objects.\footnote{Exp.~2 constitutes a replication of \citeA{GrafEtAl2016}.} We manipulated the contextual informativeness of each level of reference -- subordinate (\emph{dalmatian}), basic (\emph{dog}), and superordinate (\emph{animal}) -- by manipulating the distractor items. 

\subsubsection{Method}

\paragraph{Participants}

We recruited 58 pairs of participants (116 participants total) over Amazon's Mechanical Turk who were each paid \$1.75 for their participation. 

\paragraph{Procedure and materials}

The procedure was identical to that of Exp.~1. Participants proceeded through 72 trials. Of these, half were critical trials of interest and half were filler trials (the critical trials from Exp.~1). On critical trials, we varied the level of reference that was sufficient to mention for uniquely establishing reference.

Stimuli were selected from nine distinct domains, each corresponding to distinct basic level categories such as \emph{dog}.  For each domain, we selected four subcategories to form our target set (e.g. \emph{dalmatian}, \emph{pug}, \emph{German Shepherd} and \emph{husky}). See \tableref{tab:reflevelstimuli} for a full list of domains and their associated target items. Each domain also contained an additional item which belonged to the same basic level category as the target (e.g., \emph{greyhound}) and items which belonged to the same supercategory but not the same basic level (e.g., \emph{elephant} or \emph{squirrel}). The latter items were used as distractors.


\begin{table}
\centering
\caption{List of domains and associated superordinate category, target stimuli, and mean length (standard deviation) in characters of actually produced subordinate level utterances in Exp.~2.}
	\label{tab:reflevelstimuli}
	\begin{tabular}{l l l l}
	\toprule
	Domain & Super & Targets & Mean sub length (sd)\\
	\midrule
	\multirow{4}{*}{bear} & \multirow{4}{*}{animal} & black bear & 9.9 (.14)\\
	& & polar bear & 8.8 (.35)\\
	& & panda bear & 5.5 (.2)\\
	& & grizzly bear & 9 (.98)\\
	\midrule
	\multirow{4}{*}{bird} & \multirow{4}{*}{animal} & eagle & 4.9 (.1)\\
	& 	& parrot & 6.1 (.13)\\
	& & pigeon & 5.9 (.22)\\
	& 	& hummingbird & 10.1 (.5)\\
	\midrule
	\multirow{4}{*}{candy} & \multirow{4}{*}{snack} & MnMs & 4.4 (.49)\\
		& & skittles & 6.9 (.43)\\
		& & gummy bears & 8.5 (.47)\\
		& & jelly beans & 9.3 (.44)\\
	\midrule
	\multirow{4}{*}{car} & \multirow{4}{*}{vehicle} & SUV & 3 (0)\\
		& & minivan & 5.7 (.27)\\
		& & sports car & 9.8 (.23)\\
		& & convertible & 11.1 (.2)\\
	\midrule
	\multirow{4}{*}{dog} & \multirow{4}{*}{animal} & pug & 3 (.08)\\
		& & husky & 4.7 (.22)\\
		& & dalmatian & 8.8 (.18)\\
		& & German Shepherd & 13.1 (.82)\\
	\midrule
	\multirow{4}{*}{fish} & \multirow{4}{*}{animal} & catfish & 6.6 (.4)\\
		& & goldfish & 7.9 (.22)\\
		& & swordfish & 8 (.43)\\
		& & clownfish & 9.1 (.38)\\
	\midrule
	\multirow{4}{*}{flower} & \multirow{4}{*}{plant} & rose & 4 (0)\\
		& & tulip & 4.4 (.18)\\
		& & daisy & 5.9 (.55)\\
		& & sunflower & 9 (.11)\\
	\midrule
	\multirow{4}{*}{shirt} & \multirow{4}{*}{clothing} & T-shirt & 6.4 (.48)\\
		& & polo shirt & 6.7 (.79)\\
		& & dress shirt & 11 (0)\\
		& & Hawaii shirt & 12.6 (.46)\\
	\midrule
	\multirow{4}{*}{table} & \multirow{4}{*}{furniture} & picnic table & 9.7 (.58)\\
		& & dining table & 12 (0)\\
		& & coffee table & 9.1 (.95)\\
		& & bedside table & 8.3 (.68)\\				
	\bottomrule
	\end{tabular}
\end{table}
Each trial consisted of a display of three images, one of which was designated as the target object. Each pair of participants saw each target exactly once, for a total of 36 trials per pair. These target items were randomly assigned distractor items which were selected from four different context conditions, corresponding to different communicative pressures (see \figref{fig:dogcontexts}). We refer to these conditions with pairs of numerals specifying which levels of the taxonomy are present in the distractors: (a) item12 contexts contain one distractor of the same basic level and one distractor of the same superlevel (e.g., target: \emph{dalmatian}, distractor 1: \emph{greyhound} (also a dog), distractor 2: \emph{squirrel} (also an animal)); (b) item22 contexts contain two distractors of the same superlevel but different basic level as the target (e.g., target: \emph{husky}, distractors: \emph{hamster} and \emph{elephant}); (c) item23 contexts contain one distractor of the same superlevel and one unrelated item (e.g., target: \emph{pug}, distractor 1: \emph{cow}, distractor 2: \emph{table}); and (d) item33 contexts contain two unrelated items (e.g., target: \emph{German Shepherd}, distractors: \emph{shirt} and \emph{cookie}). 

This context manipulation served as a manipulation of utterance informativeness: any target could be referred to at the sub (\emph{dalmatian}), basic (\emph{dog}) or super (\emph{animal}) level. However, the level of reference necessary for uniquely referring differed across contexts: in item12 contexts, the sub level was necessary. In item22 and item23 contexts, the basic level was necessary (though the sub level was also possible). In item33 contexts all three utterances were possible. %Utterance probabilities for a speaker with the desire to be sufficiently informative in context but with no additional preference for referring at any particular level are shown in \figref{fig:reflevelresults} alongside empirical utterance proportions.


\subsubsection{Data pre-processing and exclusion}

We collected 2187 referential expressions. To determine the level of reference for each trial, we followed the following procedure. First, 41 trials on which the listener selected the wrong referent were excluded, leading to the elimination of 1.9\% of trials. Then, speakers' and listeners' messages were parsed automatically; the referential expression used by the speaker was extracted for each trial and checked for whether it contained the current target's correct sub, basic or super level term using a simple grep search. In this way, 72.1\% of trials were labelled as mentioning a pre-coded level of reference. In the next step, remaining utterances were checked manually to determine whether they contained a correct level of reference term which was not detected by the grep search due to typos or grammatical modification of the expression. In this way, meaning-equivalent alternatives such as \emph{doggie} for \emph{dog}, or reduced forms such as \emph{gummi}, \emph{gummies} and \emph{bears} for \emph{gummy bears} were counted as containing the corresponding level of reference term. This covered another 15.1\% of trials. A total of 12.8\% of correct trials were excluded because the utterance consisted only of an attribute of the superclass (\emph{the living thing} for \emph{animal}), of the basic level (\emph{can fly} for \emph{bird}), of the subcategory (\emph{barks} for \emph{dog}) or of the particular instance (\emph{the thing facing left}) rather than a category noun. These kinds of attributes were also mentioned in addition to the noun on trials which were included in the analysis for 8.9\% of sub level terms, 19.1\% of basic level terms, and 66.7\% of super level terms. On 1.2\% of trials two different levels of reference were mentioned; in this case the more specific level of reference was counted as being mentioned in this trial. After all exclusion and pre-processing, 1870 cases classified as one of \emph{sub}, \emph{basic}, or \emph{super} entered into the analysis.



\subsubsection{Results and discussion}

Proportions of sub, basic, and super level utterances are shown in the top row of \figref{fig:exp2results}. Overall, super level mentions are highly dispreferred ($< 2\%$), so we focus in this section only on predictors of sub over basic level mentions. The clearest pattern of note is that sub level mentions are only preferred in the most constrained context that necessitates the sub level mention for unique reference (item12, e.g. target: dalmatian, distractor: greyhound). Nevertheless, even in these contexts there is a non-negligible proportion of basic level mentions (28\%). In the remaining contexts, where the sub and basic level are equally informative, there is a clear preference for the basic level. 


\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{../../../models/5d_bda_nom_full/graphs/qualitativepattern-fulldataset-typicalities-hmc}
\caption{Utterance probabilities across different conditions. Columns indicate utterances, rows indicate data type (empirical proportion, MAP estimates of posterior predictives for full model with cost and non-deterministic semantics.}
\label{fig:exp2results}
\end{figure}

What explains these preferences? In order to test for effects of informativeness, length, frequency, and typicality on nominal choice we conducted a mixed effects logistic regression predicting sub over basic level mention from centered predictors for the factors of interest and the maximal random effects structure that allowed the model to converge (random by-speaker and by-target intercepts). 

\emph{Frequency} was coded as the difference between the sub and the basic level's log frequency, as extracted from the Google Books Ngram English corpus ranging from 1960 to 2008. 

\emph{Length} was coded as the ratio of the sub to the basic level's length. We used the mean empirical lengths in characters of the utterances participants produced. For example, the minivan, when referred to at the subcategory level, was sometimes called ``minivan'' and sometimes ``van'' leading to a mean empirical length of 5.71. This is the value that was used, rather than 7, the length of ``minivan''. That is, a higher frequency difference indicates a \emph{lower} cost for the sub level term compared to the basic level, while a higher length ratio reflects a \emph{higher} cost for the sub level term compared to the basic level.\footnote{We replicate the well-documented negative correlation between length and log frequency ($r = -.49$ in our dataset).} 

\emph{Typicality} was coded as the ratio of the target's sub to basic level label typicality.\footnote{Typicalities were elicited in a separate norming study that was identical in procedure to that of Exp.~1a. See \appref{app:typicalitynorms2} for details about the study.} That is, the higher the ratio, the more typical the object was for the sub level label compared to the basic level; or in other words, a higher ratio indicates that the object was relatively atypical for the basic label compared to the sub label. For instance, the panda was relatively atypical for its basic level ``bear'' (mean rating 0.75) compared to the sub level term ``panda bear'' (mean rating 0.98), which resulted in a relatively \emph{high} typicality ratio.

\emph{Informativeness} condition was coded as a three-level factor: \emph{sub necessary}, \emph{basic sufficient}, and \emph{super sufficient}, where item22 and item23 were collapsed into \emph{basic sufficient}. Condition was Helmert-coded: two contrasts over the three condition levels were included in the model, comparing each level against the mean of the remaining levels (in order: \emph{sub necessary}, \emph{basic sufficient}, \emph{super sufficient}). This allowed us to determine whether the probabilities of type mention  for neighboring conditions were significantly different from each other, as suggested by \figref{fig:exp2results}.

%\begin{table}[!tbp]
%\begin{center}
%\begin{tabular}{lrrl}
%\toprule
%\multicolumn{1}{l}{}&\multicolumn{1}{c}{Coef $\beta$}&\multicolumn{1}{c}{SE($\beta$)}&\multicolumn{1}{c}{$p$}\tabularnewline
%\midrule
%Intercept&$-0.02$&$0.27$&\textgreater 0.93\tabularnewline
%Condition sub.vs.rest&$ 2.05$&$0.17$&\textbf{\textless .0001}\tabularnewline
%Condition basic.vs.super&$ 0.54$&$0.15$&\textbf{\textless .001}\tabularnewline
%Frequency&$ 0.07$&$0.10$&\textgreater 0.5\tabularnewline
%Length&$-0.95$&$0.27$&\textbf{\textless .001}\tabularnewline
%Typicality&$ 4.84$&$1.32$&\textbf{\textless .001}\tabularnewline
%\bottomrule
%\end{tabular}\end{center}
%
%\end{table}

The log odds of mentioning the sub level term were greater in the \emph{sub necessary} condition than in either of the other two conditions ($\beta = 2.05$, $SE = .17$, $p < .0001$), and greater in the \emph{basic sufficient} condition than in the \emph{super sufficient} condition ($\beta = .54$, $SE = .15$, $p < .001$), suggesting that the contextual informativeness of the sub level mention has a gradient effect on utterance choice.\footnote{Importantly, model comparison between the reported model and one that subsumes basic and super under the same factor level revealed that the three-level condition variable is justified ($\chi ^2 (1) = 12.82$, $p < .0004$), suggesting that participants don't simply revert to the basic level unless contextually forced not to.} There was also a main effect of typicality, such that the sub level term was preferred for objects that were more typical for the sub level compared to the basic level  description ($\beta = .4.84$, $SE = 1.32$, $p < .001$, see \figref{fig:exp2effects}). In addition, there was a main effect of length, such that as the length of the sub level term increased compared to the basic level term (``chihuahua''/``dog'' vs.~``pug''/``dog''), the sub level term was dispreferred (``chihuahua'' is dispreferred compared to ``pug'', $\beta = -.95$, $SE = .27$, $p < .001$, see \figref{fig:exp2effects}). The main effect of frequency did not reach significance ($\beta = .07$, $SE = .10$, $p < .51$).

\begin{figure}
\centering
\includegraphics[width=\textwidth]{pics/lengthtypicality}
\caption{Proportion of sub level (over sub and basic level) terms across conditions. Left: when the sub  length is relatively short (.67,1.82] or long [1.82,4.3) compared to the basic level term. Right: when the target object was relatively more [1.06,1.91) or less (.88,1.06] typical for the sub compared to the basic level term. Intervals were generated by splitting data into groups of roughly equal numbers of observations.}
\label{fig:exp2effects}
\end{figure}


Unsurprisingly, there was also significant by-participant and by-domain variation in sub level term mention. %\figref{fig:bigscatterplot} shows the by-domain variation in utterance choice. 
For instance, mentioning the sub over the basic level term was preferred more in some domains (e.g. in the ``candy'' domain) than in others. Likewise, some domains had a greater preference for basic level terms (e.g. the ``shirt'' domain). Using the super term also ranged from hardly being observable (e.g. the ``flower'' domain) to being used more frequently (e.g. in the ``table'' and ``car'' domain). 

We thus replicate the well-documented preference to refer to objects at the basic level, which is partly modulated by contextual informativeness and partly a result of the basic level term's cognitive cost and typicality compared to its sub level competitor. 

Perhaps surprisingly given the previous literature, we did not observe an effect of frequency on sub level term mention. This may have a number of reasons. For instance, the modality of the experiment may have mattered here: the current study was a written production study, while most studies that have identified frequency as a factor governing production choices are spoken production studies (\red{cite cite}). It may be that the cognitive cost of typing longer words may be disproportionately higher than that of producing longer words in speech, thus obscuring a potential effect of frequency. 

\subsection{Non-deterministic RSA for nominal choice}
\label{sec:reflevelmodel}

Here we show that non-deterministic RSA as presented in \sectionref{sec:modifiedmodel} can be straightforwardly extended to modeling the choice of taxonomic level of reference. We include three modifications, while leaving the general framework as is. The first modification concerns the utterance alternatives. The second concerns the elicited typicality values and the resulting fidelity values. The third concerns the cost function. We briefly elaborate on each in turn. 

\paragraph{Utterance alternatives.} Whereas the modifier choice model treats all individual features and feature combinations represented in the display as utterance alternatives, the nominal choice model considers only the three different levels of reference to the target as alternatives, e.g., \emph{dalmatian}, \emph{dog}, \emph{animal}. That is, assuming a German Shepherd as a distractor, \emph{German Shepherd} is \emph{not} considered an alternative. This has consequences for the assumed fidelity values, which we turn to next. \jd{we should probably discuss this in the GD? ie, if we also assumed distractor labels as alternatives, we would have to do the rescaling -- would results be different? or the othe rway round: if we assume in modifier choice only the target's features are available as alternatives, would results be different?}

\paragraph{Fidelity values.} Just as we did for capturing color typicality effects in \sectionref{sec:modelevalcolortypicality}, we elicited empirical typicality values for object-utterance combinations.\footnote{See \appref{app:typicalitynorms2} for details of typicality elicitation experiment.} For each display, we know the typicality of each object in the display as an instance of the three potential target utterances (capturing, for instance, that the word ``dog'' describes a dalmatian better than a grizzly bear, but it also describes a grizzly bear better than a tennis ball). This allows us to use the typicality values as fidelity values directly, without rescaling as was necessary in the modifier choice model.

\paragraph{Cost function.} Recall the pragmatic speaker's utility function from \sectionref{sec:modifiedmodel}, where the weighted informativeness term $\lambda \ln P_{L_0}(o | u)$ traded off against the weighted utterance cost $\beta_c c(u)$. In the modifier model we assumed a constant cost for each added modifier. Because all utterance alternatives in the nominal choice model have word length 1, we update the cost function to be composed of each utterance's length $\hat{c}_l$ and frequency $\hat{c}_f$ (as described in the previous section), weighted by free parameters $\beta_f$ and $\beta_l$ :
\begin{equation}
P_{S_1}(u | o) \propto e^{\lambda \ln P_{L_0}(o | u) + \beta_f \hat{c}_f  + \beta_l \hat{c}_l)}
\end{equation}

To understand the qualitative behavior of the model, we briefly delve into two aspects of the model: first, the effect of typicality on the literal listener (and, in consequence via the pressure to be informative) the speaker. And second, the effect of cost (utterance length and frequency) on the speaker.

\subsubsection{Typicality effects}

\paragraph{Literal listener behavior.} The literal listener's probability of choosing the target under different typicalities for the observed utterance are shown in \figref{fig:nominallistenertypicality}. In general: as the target's typicality as an instance of the utterance increases and the distractors' typicality decreases, the probability of the literal listener choosing the target increases. Subordinate level terms tend to fall in the upper right quadrant of this graph. Basic level terms in the \emph{sub necessary} conditions tend to fall in the lower right quadrant, while basic level terms in the \emph{basic sufficient} conditions tend to fall in the upper right quadrant as well.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{../../../models/6_qualitative_nom_L0/L0probs.png}
\caption{Literal listener probability of choosing the target under different typicalities of the target (x-axis) or the distractors (color) for the observed utterance. For simplicity we assume equal typicality of both distractors. The remaining probability mass for each case is thus uniformly distributed over both distractors.}
\label{fig:nominallistenertypicality}
\end{figure}

\paragraph{Pragmatic speaker behavior.} To understand the effect of typicality on the speaker's behavior it is useful to think about the problem of deciding which taxonomic level to refer at in terms of typicality gain, as we did in \sectionref{sec:colortypicality} for the choice between modified and unmodified expression. There, we found that relatively large target (compared to distractor) typicality gains in going from unmodified to modified expressions compared resulted in greater probability of overmodification. Here we observe the same effect in going from a higher (less specific) to a lower (more specific) taxonomic level. This can be seen in \figref{fig:nominalspeakertypicality}, which shows the probability of each utterance (sub, basic, or super) as a function of absolute target typicality as well as target typicality gain. Target typicality gain is the difference between the target's sub level typicality and the target's basic level typicality. Probabilities are shown for contexts with three items, always assuming $\lambda = 7$, but manipulating distractor typicality to simulate conditions analogous to our experimental conditions \emph{sub necessary}, \emph{basic sufficient}, and \emph{super sufficient}. Simulated distractor typicalities for sub, basic, and super level reference are shown in \tableref{tab:simulatedtyps}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../../models/7_qualitative_nom_S1_typ/S1probs_typgainmap_alpha7.png}
\caption{Pragmatic speaker probability of choosing each utterance (sub, basic, super) under varying absolute target sub typicalities (x-axis) and target typicality gains (y-axis), assuming equal typicality values for both distractors. Rows indicate different simulated conditions.}
\label{fig:nominalspeakertypicality}
\end{figure}

\begin{table}
\centering
\caption{Simulated distractor typicality (fidelity) values for sub, basic, and super level utterances in simulated conditions. In contrast to the actual experimental conditions, we assume equal typicality values for both distractors.}
\begin{tabular}{l l c c c}
\toprule
& & \multicolumn{3}{c}{Condition}\\
& & sub necessary & basic sufficient & super sufficient\\
\midrule
\multirow{3}{*}{Utterance} & sub & 0 & 0 & 0 \\
& basic & .8 & .1 & 0 \\
& super & .8 & .8 & 0\\
\bottomrule
\end{tabular}
\label{tab:simulatedtyps}
\end{table}

The blue areas in the graph indicate highest-probability regions. For example, as expected in the \emph{sub necessary} condition, the sub level term is the most likely one. However, in certain cases the basic level term also receives non-zero probability, notably when the target is a better instance of the basic than the sub level term, or (not pictured) when the typicality of the distractor as an instance of the basic level term is very low  (e.g., the typicality of the koala bear as an instance of "bear" was only 0.50). Indeed, the grizzly (with high typicality for basic level ``bear'', .97) is referred to as ``bear'' rather than ``grizzly bear'' in 85\% of \emph{sub necessary} conditions when the koala is the distractor. 

In the \emph{basic sufficient} conditions, sub level reference is nevertheless strongly predicted when target sub typicality gain is positive (i.e., when the target is a much better instance of the sub than of the basic level term). An example of such a case is the panda bear, who received a sub level typicality of .98 and a basic level typicality of only .75. Indeed, even when basic level reference was sufficient, the panda was referred to as the ``panda'' 81\% of the time.

These patterns mirror the typicality effects obtained via the mixed effects regression.


\subsubsection{Cost effects}

The additional effect of cost on nominal choice is straightforward: the costlier an utterance (relative to its alternatives), the less likely it is to be used. This pattern, too, is one observed in the mixed effects regression. For instance, the (short, less costly) pug is almost three times as likely as the (long, more costly) German Shepherd to be referred to by its subordinate level term in the \emph{basic sufficient} and \emph{super sufficient} conditions, where subordinate level reference is unnecessary. 

\bigskip
In \sectionref{sec:reflevelmodel} we showed that non-deterministic RSA captures the right kinds of qualitative effects as observed in the mixed effects regression. In the next section we evaluate how well the model captures nominal choice preferences quantitatively.

\subsection{Model evaluation: nominal choice}
\label{sec:reflevelmodeleval}

In order to evaluate non-deterministic RSA for nominal choice, we repeated the same Bayesian data analysis as reported in \sectionref{sec:modifiermodeleval} and \sectionref{sec:modelevalcolortypicality} to generate model predictions and infer likely parameter values. We did so by conditioning on the observed production data (coded into \emph{sub}, \emph{basic}, and \emph{super} level mentions as described above) and integrating over the three free parameters $\lambda  \sim \mathcal{U}(0,20)$, $\beta_f  \sim \mathcal{U}(0,5)$, $\beta_l  \sim \mathcal{U}(0,5)$.


Point-wise maximum a posteriori (MAP) estimates of the model's posterior predictives for each combination of utterance and informativeness condition (collapsing across different items) are compared to empirical data in \figref{fig:exp2results}. The model clearly captures the preference towards sub level mentions in the \emph{sub necessary} conditions and the basic level preference in all other conditions. It also captures the further decrease in sub level mentions in the \emph{super sufficient condition}. However, it does overpredict super level mentions, though not as badly as models that either assume a deterministic semantics or that ignore utterance cost.\footnote{The reader is referred to \appref{app:nominalmodelcomparison} for a comparison of the models containing a) only informativeness with deterministic semantics; b) only informativeness with non-deterministic semantics; c) informativeness with deterministic semantics and cost; d) informativeness with non-deterministic semantics and cost (the current model).} At this level, the model achieves a correlation of $r = .94$. Computing correlations additionally on the by-target level yields a correlation of $r = .84$ (see also the scatterplot in \figref{fig:exp2scatter}). 

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{../../../models/5d_bda_nom_full/graphs/scatterplot-fulldataset-typicalities-hmc}
\caption{Scatterplot of by-target empirical utterance proportions against model posterior predictive MAP estimates. Gray line indicates perfect correlation line.}
\label{fig:exp2scatter}
\end{figure}

Parameter posteriors are shown in \figref{fig:nominalparamposteriors}. Both informativeness and length receive significant weight. In contrast, the effect of frequency appears to be much weaker with a MAP of .1 and the HDIs overlapping with 0. This mirrors the null effect of frequency found in the regression analysis. However, a large number of cases also received a non-zero frequency weight.

In order to ascertain whether typicality as incorporated in the non-deterministic semantics was indeed contributing to the explanatory power of the model, we ran an additional Bayesian data analysis with an added typicality weight parameter $\beta_t \in [0, 1]$. This parameter interpolated between empirical typicality values (when $\beta_t = 1$) and deterministic (i.e., 0 or 1) a priori values based on the true taxonomy (when $\beta_t = 0$). We found a MAP estimate for $\beta_t$ of .95, HDI = [0.82,.99], strongly indicating that it is useful to incorporate empirical typicality values and thus providing further support for the value of non-deterministic truth functions in modeling referential expressions.

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{../../../models/5d_bda_nom_full/graphs/parameterposteriors-fulldataset-typicalities-hmc}
\caption{Posterior distribution over model parameters. Maximum a posteriori (MAP)  $\beta_f$ = 0.10, 95\% highest density interval (HDI) = [0.002,0.95]; MAP $\beta_l$ = 1.85, HDI = [1.23,2.65]; MAP $\lambda$ = 9.19, HDI = [7.72,10.80].}
\label{fig:nominalparamposteriors}
\end{figure}


%This relationship is motivated by considering the effect of a small difference in typicality on choice probability: in our elicitation experiment a small difference in rating should mean the same thing at the top and bottom of the scale (it is visually equivalent on the slider that participants used). In order for a small difference in typicality rating to have a constant effect on relative choice probability (which is a ratio), the relationship must be exponential. 


\section{General Discussion}
\label{sec:gd}

Important sub-sections:
\begin{itemize}
	\item nature of numbers in non-deterministic semantics (we see that typicality effects fall out of treating the numbers in the semantics as typicality values for noun choice, and the same if we make up values for modifier choice -- but how do we get here compositionally? one would ideally model the structure in the prior (rich world knowledge), ie the statistical correlations between different dimensions (type -- banana, color -- blue/yellow, etc) and have the fidelity/noise values emerge from THAT compositionally!
	\item talk about how our approach fits in with incremental stories of overspecification?
	\item The work reported here clearly shows that overmodified referring expressions, contrary to some claims in the literature \cite{Engelhardt2011} \red{who say overmodification ipmairs comprehension}, contribute more utility than `minimally' specified referring expressions.
	\item speaker vs listener utility? audience design/perspective-taking?
	\item how the approach applies to other referring expressions like those mentioned in the intro (indefinite, post-nominal modification, names, pronouns) -- and to language production in general: language is noisy, so it's helpful to be redundant IN THE RIGHT KIND OF WAY. communicative efficiency! cite florian and roger and ted and steve. 
\end{itemize}
%
\jd{There's a parallel to be made between typicality effects on ``overmodification" and the literature on instrument mention. For example, \cite{brown1987} showed that atypical instruments are more likely to be mentioned than typical ones ("The man was stabbed with an ice pick", not "The man was stabbed with a knife"). Their account of the effect is that speakers do this for speaker-internal reasons. \cite{lockridge2002} replicated the original finding in a story retelling scenario, but also manipulated whether or not the addressees saw pictures of the actions. Without pictures, there were even more mentions of atypical objects, which they take as evidence that the typicality effect is an audience design effect. THIS IS COOL AND PICKS UP ON THE SPEAKER-OR-LISTENER POINT YOU RAISE IN THE INTRO IN THE TYPICALITY PART!!!!!}
%
% gary lupyan triangle prototypicality study

\appendix

\section{Effects of fidelity on utterance probabilities}
\label{app:modelexploration}

Here we visualize the effect of fidelity on the probability of producing the simple insufficient, simple sufficient, or complex redundant referring expression to refer to the target in contexts like  \figref{fig:sizesufficient} and \figref{fig:colorsufficient}, under varying $\lambda$ values, in \figref{fig:fullexploration}. This constitutes a generalization of \figref{fig:basicasymmetry}, which is duplicated in row 6.

\begin{figure}
\includegraphics[width=\textwidth]{pics/modelexploration-fullfidelityeffect-unlogged-wide}
\caption{Utterance probability as a function of sufficient and insufficient utterance fidelities (x-axis, colors) and varying $\lambda$ (rows).}
\label{fig:fullexploration}
\end{figure}

\section{Model exploration for Koolen scene variation contexts}
\label{app:koolenexploration}

In \figref{fig:koolenfullexploration} we visualize model predicted probability of redundantly using color under varying $\lambda$ values (columns), color fidelity values (rows), and size fidelity values (x-axis), for the high and low variation conditions in their Exp.~1 (where type was sufficient for reference) and Exp.~2 (where type and size was necessary for reference). The assumed type fidelity is .9.

\begin{figure}
\includegraphics[width=\textwidth]{pics/koolen-full-exploration}
\caption{Probability of redundant color mention as a function of size fidelity (x-axis), color fidelity (rows), and varying $\lambda$ (columns).}
\label{fig:koolenfullfullexploration}
\end{figure}


\section{Validation of interactive web-based written production paradigm}
\label{app:replication}

\red{make sure to discuss why overall we have lower overspecification rates -- probably because of color typicality!! we had pretty typical colors in our stimuli}

\section{Pre-experiment quiz}
\label{app:numdistractors}

Before continuing to the main experiment, each participant had to correctly respond ``True'' or ``False'' to the following statements. Correct answers are given in parentheses after the statement.

\begin{itemize}
	\item The speaker can click on an object. (False)
	\item The listener wants to click on the object that the speaker is
  telling them about. (True)
  \item  The target is the object which has the red circle around it. (False)
  \item Only the speaker can send messages. (False)
  \item There are a total of 72 rounds. (True)
  \item The locations of the three objects are the same for the speaker and the listener. (False)
\end{itemize}


\section{Item types}
\label{app:itemtypes}

The following table lists all 36 object types from Exp.~\red{XXX} and the colors they appeared in:

\begin{tabular}{l l l l}
\toprule
Object & Colors & Object & Colors \\
\midrule
avocado & black, green & balloon & pink, yellow \\
belt & black, brown & bike & purple, red\\
billiard ball & orange, purple & binder & blue, green \\
book & black, blue & bracelet & green, purple \\
bucket & pink, red & butterfly & blue, purple\\
candle & blue, red & cap & blue, orange \\
chair & green, red & coat hanger & orange, purple \\
comb & black, blue & cushion & blue, orange\\
flower & purple, red & frame & green, pink \\
golf ball & blue, pink & guitar & blue, green\\
hair dryer & pink, purple & jacket & brown, green\\
napkin & orange, yellow & ornament & blue, purple\\
pepper & green, red & phone & pink, white\\
rock & green, purple & rug & blue, purple \\
shoe & white, yellow & stapler & purple, red\\
thumb tack & blue, red & tea cup & pink, white \\
toothbrush & blue, red & turtle & black, brown \\
wedding cake & pink, white & yarn & purple, red\\
\bottomrule
\end{tabular}

%\section{Analysis of cases with higher inferred size than color typicality}
%\label{app:fidelity-outliers}
%
%\figref{fig:fidelity-outliers-reducedconditions} visualizes all 10,000 samples' from the BDA run on the data from Exp.~1 with an eye towards understanding the cases in which inferred size fidelity was greater than color fidelity. In general, these cases were very unlikely, with a posterior probability of .048. Conditioning on these cases, the probability of a low cost weight was only .23 (only .01 probability overall). That is, where color fidelity was lower than size fidelity, the model tried to achieve the empirical color-size asymmetry by placing greater weight on cost differences, where size was always inferred to be costlier than color.
%
%\begin{figure}
%\centering
%\includegraphics[width=.7\textwidth]{../../../models/1a_bda_basic/results_bda/graphs/fidelity-outliers-fixed-reducedconditions}
%\caption{Each sample's cost weight $\beta_c$ against difference between color and size fidelity ($f_{\textrm{c}} - f_{\textrm{s}}$). Green region includes cases with higher inferred size than color fidelity that have a cost weight greater than or equal to the median cost weight. Red region includes cases with higher inferred size than color fidelity that have a cost weight lower than the median cost weight. Dot size and color indicates parameter values' posterior probability.}
%\label{fig:fidelity-outliers-reducedconditions}
%\end{figure}

\section{Experiment 2a: typicality norms for Experiment 2}
\label{app:typicalitynorms2}

Analogous to the color typicality norms elicited for utterances in Exp.~1, we elicited typicality norms for utterances in Exp.~2. The elicited typicalities were used in the Bayesian Data Analysis reported in \sectionref{sec:reflevelmodeleval}.

\subsubsection{Methods}

\paragraph{Participants}

We recruited 240 participants over Amazon's Mechanical Turk who were each paid \$0.50 for their participation.

\paragraph{Procedure and materials}

On each trial, participants saw one of the images used in Exp.~2 and were asked to answer the question ``How typical is this for an \emph{X}?'' on a continuous slider with endpoints labeled ``very atypical'' to ``very typical.'' \emph{X} was a nominal referring expression. In contrast to Exp.~1a, where we only elicited typicality norms for utterance-object pairs where the object was in the extension of the utterance under a deterministic semantics (e.g., here\emph{dalmatian}, \emph{dog}, or \emph{animal} for a dalmatian), in this norming study we also elicited norms for utterance-object pairs where that was not clearly the case (e.g., \emph{a bear} for a bison, \emph{a car} for an ambulance, or \emph{a snack} for a lobster). However, we did not test all utterance-object combinations, which would have led to an explosion of conditions. Instead, we tested each target object with its three utterances (e.g., the dalamtian was paired with \emph{dalmatian}, \emph{dog}, and \emph{animal}; the pug was paired with \emph{pug}, \emph{dog}, and \emph{animal}, etc.). That yielded a total of 108 combinations -- four targets in nine domains with three utterances each. We further tested each distractor item that shared the target's superclass category (\emph{dist-samesuper}, e.g., cows share the superclass category animal with dogs) on both the basic level and the super level term (e.g., \emph{dog} for cow and \emph{animal} for cow), for a total of 469 combinations. Finally, we also tested each distractor of a different super category than the target on the target's super level term (\emph{dist-diffsuper}, e.g., \emph{animal} for socks). This yielded another 168 combinations. Overall, we obtained typicality norms for 745 object-utterance combinations. All other object-utterance combinations were assumed to have typicality 0.

Each participant rated 45 items: 7 targets, 10 dist-diffsuper, and 28 dist-samesuper cases. These were randomly sampled from the overall pool of items in each category. 

\subsubsection{Results and discussion}

Each combination was rated at least 5 times and at most 27 times. We coded the slider endpoints as 0 (``very atypical'') and 1 (``very typical''). In order to evaluate the model, we used each object-utterance combination's typicality mean as input. 

Typicality ratings by item type (target, dist-samesuper, dist-diffsuper) and utterance type (sub, basic, super) are visualized in \figref{fig:typicalityboxplots}. As expected, typicality was close to 0 for dist-diffsuper cases and for sub/basic terms used with dist-samesuper cases. However, even for these cases, there was some variation. 

For targets, typicality of the object for the utterance decreased with increasing reference level, mirroring the typicality ratings obtained for Exp.~1 -- a particular object is a better instance of the more specific term than of the more general term for that object.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{pics/typicalityboxplot}
\caption{Boxplots of typicality ratings. The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles). Upper and lower whiskers extend from the respective hinge to the highest and lowest values that are within 1.5 times the inter-quartile range of the hinge. Outliers are indicated as gray dots.}
\label{fig:typicalityboxplots}
\end{figure}

\section{Nominal choice model comparison}
\label{app:nominalmodelcomparison}

% a) det-nocost: 500 samples, 200 burn
% b) det-cost: 1000 samples, 300 burn
% c) nondet-nocost: 1200 samples, 600 burn -- NOT YET RUN
% d) nondet-cost: 2000 samples, 1000 burn -- CURRENTLY RUNNING
% e) interpolation:

\jd{This isn't model comparison in the technical sense, just a side-by-side look at the different models. Leave it in or throw out?}

Here we report correlations, MAP estimates of posterior predictives collapsed across targets and items, and scatterplots of posterior predictive MAP estimates on the by-target level for the model containing a) only informativeness with deterministic semantics; b) informativeness with deterministic semantics and cost; c) only informativeness with non-deterministic semantics; d) informativeness with non-deterministic semantics and cost (the model reported in the main text). \tableref{tab:nominalmodelcorr} shows correlations. \figref{fig:nominalmodelqual} shows the collapsed patterns for utterance choice. \figref{fig:nominalmodelscatt} shows the scatterplots.

\begin{table}
\centering
\caption{Correlations ($r$ and $R^2$) of posterior predictive MAPs of four different models (see main text) with empirical proportions of sub, basic, and super level choices.}
	\begin{tabular}{l l c c c c}
	\toprule
	& & \multicolumn{4}{c}{Model}\\
	\multicolumn{2}{l}{Semantics}  & deterministic & deterministic & non-deterministic & non-deterministic\\
	\multicolumn{2}{l}{Cost} &  no & yes & no & yes \\
	\midrule
	\multirow{2}{*}{$r$} & collapsed & .85 & .88 & .86 & .94\\
	& by-target & .63 & .71 & .71 & .84\\
	\midrule
	\multirow{2}{*}{$R^2$} & collapsed & .72 & .77 & .74 & .89\\
	& by-target & .40 & .51 & .51 & .70\\
	\bottomrule
	\end{tabular}
	\label{tab:nominalmodelcorr}
\end{table}

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{pics/qualitativepattern-complete}
\caption{Utterance probabilities across different conditions. Columns indicate utterances, rows indicate data type (empirical proportion, MAP estimates of posterior predictives for the four different models).}
\label{fig:nominalmodelqual}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{pics/scatterplot-complete}
\caption{Scatterplot of by-target empirical utterance proportions against model posterior predictive MAP estimates for the four different models. Gray line indicates perfect correlation line.}
\label{fig:nominalmodelscatt}
\end{figure}

\section{Gatt replication}
\red{report Gatt et al 2011 replication}

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{bibs}


\end{document}
